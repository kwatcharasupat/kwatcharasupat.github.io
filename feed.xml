<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://kwatcharasupat.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://kwatcharasupat.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-07-15T04:56:45+00:00</updated><id>https://kwatcharasupat.github.io/feed.xml</id><title type="html">blank</title><subtitle>An (un)sound lab cat. </subtitle><entry><title type="html">Quantifying Spatial Audio Quality Impairment</title><link href="https://kwatcharasupat.github.io/blog/2024/spauq/" rel="alternate" type="text/html" title="Quantifying Spatial Audio Quality Impairment"/><published>2024-05-10T00:00:00+00:00</published><updated>2024-05-10T00:00:00+00:00</updated><id>https://kwatcharasupat.github.io/blog/2024/spauq</id><content type="html" xml:base="https://kwatcharasupat.github.io/blog/2024/spauq/"><![CDATA[<p>Spatial audio quality is a highly multifaceted concept (see <a href="https://depositonce.tu-berlin.de/items/50b7777f-ce30-431b-b371-55b977e0f707">this</a> for a very long list of things to consider). “Geometrical” components of spatial audio quality are perhaps the least subjective aspect of spatial audio quality to quantify, yet there have been very little attempt at dealing with it since <a href="https://gitlab.inria.fr/bass-db/bss_eval">BSS Eval</a> came out almost 20(!) years ago.</p> <p>Even the geometrical component of spatial audio quality is not trivial to quantify. We resorted to only considering the interchannel time differences (ITD) and interchannel level differences (ILD) of the test signal relative to a reference signal. With this, it is actually possible to construct a signal model to isolate <em>some</em> of the spatial distortion. By using a combination of Weiner-style least-square optimization and good ol’ correlation maximization, we propose a signal decomposition method to isolate the spatial error, in terms of interchannel gain leakages and changes in relative delays, from a processed signal. These intermediates parameters can then be used as a diagnostic tool to identify the nature of the spatial distortion and to quantify the spatial quality impairment.</p> <p>Our work is open-sourced <a href="https://github.com/kwatcharasupat/spauq">here</a> both as a Python package and a CLI.</p> <div class="repo p-2 text-center"> <a href="https://github.com/kwatcharasupat/spauq"> <img class="repo-img-light w-100" alt="kwatcharasupat/spauq" src="https://github-readme-stats.vercel.app/api/pin/?username=kwatcharasupat&amp;repo=spauq&amp;theme=catppuccin_latte&amp;show_owner=false"/> <img class="repo-img-dark w-100" alt="kwatcharasupat/spauq" src="https://github-readme-stats.vercel.app/api/pin/?username=kwatcharasupat&amp;repo=spauq&amp;theme=dark&amp;show_owner=false"/> </a> </div> <blockquote> <p>Warning: There’s a lot of math in this section. I’m sorry.</p> </blockquote> <h2 id="methods">Methods</h2> <p>Let’s call the reference signal \(\mathbf{s}[n] \in \mathbb{R}^{C}\) and the test signal \(\hat{\mathbf{s}}[n] \in \mathbb{R}^{C}\). Here, \(C\) is the number of channels and \(n\) is the sample index. The signals might not be of the same length. That’s fine here.</p> <p>We model the \(c\)th channel of the test signal as a sum of all the delayed and scaled channels of the reference signal, plus some other residual noise:</p> \[\hat{s}_c[n] = \underbrace{\sum_{d=1:C} A_{cd} s_d[n - \tau_{cd}]}_{:= \tilde{s}_c[n]} + \mathbf{e}_{\text{resid}}\] <p>We can interpret \(A_{cd}\) as the gain mapping from the \(d\)th channel of the reference signal to the \(c\)th channel of the test signal, and \((\mathbf{T})_{cd} = \tau_{cd}\) as the delay mapping from the \(d\)th channel of the reference signal to the \(c\)th channel of the test signal. Ideally, we want \(A_{cc} = 1\) and \(\tau_{cc} = 0\) for all \(c\), and \(A_{cd} = 0\) for all \(d \neq c\). The term \(\tilde{\mathbf{s}}\) is effectively the spatially distorted version of the clean signal, with no other types of distortion. We cannot actually guarantee that \(e_{\text{resid}}\) will contain no spatially relevant distortions, though. (If anyone has an idea on how to fix this, drop us an email!)</p> <h3 id="objective-function">Objective Function</h3> <p>Now that we have a model for the spatial distortion, we can define an objective function to minimize the residual noise. Basically, we want to find the gain and delay mappings that minimize the difference between the test signal and the spatially distorted reference signal. This is done by minimizing the mean squared error between the test signal and the spatially distorted reference signal:</p> \[\min_{\mathbf{A}, \mathbf{T}} \sum_{\text{valid}\ n} \left\| \hat{\mathbf{s}}[n] - \tilde{\mathbf{s}}[n] \right\|_2^2\] <h3 id="optimization">Optimization</h3> <h4 id="step-1">Step 1</h4> <p>Finding the optimal gain is simple, but finding the optimal delay is not. So in practice, we could not easily do a joint optimization due to the non-convex nature of the objective function with respect to \(\mathbf{T}\). So we resorted to a very simple correlation maximization.</p> \[\tau_{cd} = \underset{-K \le \kappa \le K}{\operatorname{arg\ max}} \left| \underset{f \in \mathfrak{F}}{\mathrm{IDFT}}\left\{ \hat{S}_c[f] \cdot S^\ast_d[f] \cdot |H[f]|^2\right\}[\kappa] \right|\] <p>where \(H\) is an optional low-pass filter if your test signal is too noisy in the high end and \(K\) limits the search space.</p> <h4 id="step-2">Step 2</h4> <p>Once we have the optimal delay, we can find the optimal gain by solving a simple least-square optimization problem. This is very similar to the solution to the Wiener filter. The only differences are that (1) instead of computing an \(N\)-tap filter, we are computing a filter with no regards to the length, as long as there is only one non-zero tap at time \(\tau_{cd}\) with magnitude \(A_{cd}\); and (2) the filter is solved channel by channel.</p> <p>Now, we form the autocorrelation <em>tensor</em> \(\mathbf{R} \in \mathbb{R}^{C \times (C \times C)}\) and the cross-correlation <em>matrix</em> \(\check{\mathbf{R}} \in \mathbb{R}^{C \times C}\).</p> \[(\mathbf{R}^c)_{bd} = \sum_{n} s_b[n - \tau_{cb}]s_d[n-\tau_{cd}]\] \[(\check{\mathbf{R}})_{cd} = \sum_{n} \hat{s}_c[n]s_d[n-\tau_{cd}]\] <h4 id="step-21">Step 2.1</h4> <p>In (very) multichannel audio, sometimes we have silent channels. This makes parameter estimation very unstable. Like all problems in life, we pretend it’s not there and proceed.</p> \[\mathfrak{C}^{\perp} := \left\{c \in 1:C \mid \textstyle\sum_n |s_c[n]|^2 &lt; \epsilon\right\}\] \[\mathfrak{D}^{\perp} := \left\{d \in 1:C \mid \textstyle\sum_n |\hat{s}_d[n]|^2 &lt; \epsilon\right\}\] \[\mathbf{A}_{\mathfrak{C}^{\perp}, :} \gets \mathbf{0},\quad \mathbf{A}_{:, \mathfrak{D}^{\perp}} \gets \mathbf{0}\] <h4 id="step-3">Step 3</h4> <p>Now, we can do the highly anticipated matrix inversion.</p> \[\mathbf{A}_{c,\mathfrak{D}} = \check{\mathbf{R}}_{c, \mathfrak{D}}\left(\mathbf{R}^{c}_{\mathfrak{D},\mathfrak{D}}\right)^{-1}\] <blockquote> <p>It’s a lot less math from here.</p> </blockquote> <h3 id="error-decomposition">Error Decomposition</h3> <p>Let’s recap a little. We have</p> \[\hat{\mathbf{s}} = \tilde{\mathbf{s}} + \mathbf{e}_{\text{resid}}\] <p>but \(\tilde{\mathbf{s}}\) itself can be written as \(\tilde{\mathbf{s}} = \mathbf{s} + \mathbf{e}_{\text{spat}}\) where \(\mathbf{e}_{\text{spat}}\) is the noise that we know is definitely spatially related. Additive decomposition is not always the best way to do this, but it’s the easiest to understand. Let’s stick with this for now.</p> <p>Now, we have two types of errors: the residual error and the spatial error. This now allow us to do what the <em>source image to spatial distortion ratio (ISR)</em> from BSS Eval originally set out to do. With this, we have the SNR-style metrics for spatial distortion, <strong>Signal to Spatial Distortion Ratio (SSR)</strong>:</p> \[\text{SSR}(\hat{\mathbf{s}}; \mathbf{s}) = 10 \log_{10} \dfrac{\|\mathbf{s}\|^2}{\|\mathbf{e}_\text{spat}\|^2}\] <p>This measures the “amount” of spatial distortion in the signal, regardless of how much other types of distortion there are in the test signal.</p> <p>The SSR also comes with a cousin, the <strong>Signal to Residual Distortion Ratio (SRR)</strong>:</p> \[\text{SRR}(\hat{\mathbf{s}}; \mathbf{s}) = 10 \log_{10} \dfrac{\|\tilde{\mathbf{s}}\|^2}{\|\mathbf{e}_\text{resid}\|^2}\] <p>This measures the “amount” of residual distortion in the signal, regardless of how much spatial distortion there are in the test signal.</p> <h2 id="some-benchmarks">Some Benchmarks</h2> <p>Although this work is designed to be a purely objective (instead of perceptual) tool, in that the main contribution is in the estimation of \(\mathbf{A}\) and \(\mathbf{T}\), we still need to verify that the SSR and SRR has the properties of (1) being robust to non-spatial distortion and (2) follows some perceptual intuition for well-established understanding of spatial audio quality.</p> <h3 id="robustness-benchmark">Robustness Benchmark</h3> <p>For this we stereoify the well-known Speech dataset <a href="https://catalog.ldc.upenn.edu/LDC93S1">TIMIT</a> and test the metrics against panning, delay, filtering, and noise.</p> <p><em>Panning only</em></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/pdf/2024-05-10-spauq/pan-480.webp 480w,/assets/pdf/2024-05-10-spauq/pan-800.webp 800w,/assets/pdf/2024-05-10-spauq/pan-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/pdf/2024-05-10-spauq/pan.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><em>Panning and delay</em></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/pdf/2024-05-10-spauq/delaypan-480.webp 480w,/assets/pdf/2024-05-10-spauq/delaypan-800.webp 800w,/assets/pdf/2024-05-10-spauq/delaypan-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/pdf/2024-05-10-spauq/delaypan.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><em>Panning and low-pass filter</em></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/pdf/2024-05-10-spauq/lpfpan-480.webp 480w,/assets/pdf/2024-05-10-spauq/lpfpan-800.webp 800w,/assets/pdf/2024-05-10-spauq/lpfpan-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/pdf/2024-05-10-spauq/lpfpan.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><em>Panning and noise</em></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/pdf/2024-05-10-spauq/snrpan-480.webp 480w,/assets/pdf/2024-05-10-spauq/snrpan-800.webp 800w,/assets/pdf/2024-05-10-spauq/snrpan-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/pdf/2024-05-10-spauq/snrpan.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>We also test the system by using the <a href="https://leomccormack.github.io/sparta-site/">SPARTA</a> to spatialize TIMIT and <a href="https://zenodo.org/records/6387880">STARSS22</a> to 5 channels. For those with theoretical values, you will also see dotted lines in the figure below.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/pdf/2024-05-10-spauq/starss-480.webp 480w,/assets/pdf/2024-05-10-spauq/starss-800.webp 800w,/assets/pdf/2024-05-10-spauq/starss-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/pdf/2024-05-10-spauq/starss.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="codec-benchmark">Codec Benchmark</h3> <p>For this we use <a href="https://zenodo.org/records/3338373">MUSDB18-HQ</a> and compressed it using AAC under different stereo coding settings and test the compressed signals against the original.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/pdf/2024-05-10-spauq/aac-musdb-480.webp 480w,/assets/pdf/2024-05-10-spauq/aac-musdb-800.webp 800w,/assets/pdf/2024-05-10-spauq/aac-musdb-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/pdf/2024-05-10-spauq/aac-musdb.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>This figure is the same results as the above but we replotted-it to show the relative distortion instead of the absolute distortion. This clearly shows the tradeoff between using L/R, M/S, and joint coding as the bits are allocated between the content and the spatialization.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/pdf/2024-05-10-spauq/aac-musdb-rel-480.webp 480w,/assets/pdf/2024-05-10-spauq/aac-musdb-rel-800.webp 800w,/assets/pdf/2024-05-10-spauq/aac-musdb-rel-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/pdf/2024-05-10-spauq/aac-musdb-rel.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="conclusion">Conclusion</h2> <p>In this work, we propose a decomposition technique (that’s actually quite simple) to isolate the spatial distortion parameters, namely, the gain mapping and delay mapping matrices. Using these parameters, a number of downstream metrics can be proposed. We demonstrate two very simple energy-ratio metrics and benchmarked it against a number of spatial distortion scenarios.</p>]]></content><author><name></name></author><category term="research"/><category term="spatial-audio"/><category term="dsp"/><summary type="html"><![CDATA[what to do when you have no spatial awareness]]></summary></entry><entry><title type="html">Latte: Cross-framework Python Package for Evaluation of Latent-Based Generative Models</title><link href="https://kwatcharasupat.github.io/blog/2022/latte-cross-framework-python-package-for-evaluation-of-latent-based-generative-models/" rel="alternate" type="text/html" title="Latte: Cross-framework Python Package for Evaluation of Latent-Based Generative Models"/><published>2022-02-25T18:44:34+00:00</published><updated>2022-02-25T18:44:34+00:00</updated><id>https://kwatcharasupat.github.io/blog/2022/latte-cross-framework-python-package-for-evaluation-of-latent-based-generative-models</id><content type="html" xml:base="https://kwatcharasupat.github.io/blog/2022/latte-cross-framework-python-package-for-evaluation-of-latent-based-generative-models/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[by Karn N. Watcharasupat and Junyoung Lee Controllable deep generative models have promising applications in various fields such as computer vision, natural language processing, or music. However, implementations of evaluation metrics for these generative models remain non-standardized. Evaluating disentanglement learning, in particular, might require implementing your own metrics, possibly entangling you more than when you &#8230; Continue reading Latte: Cross-framework Python Package for Evaluation of Latent-Based Generative Models The post Latte: Cross-framework Python Package for Evaluation of Latent-Based Generative Models first appeared on Music Informatics Group.]]></summary></entry></feed>