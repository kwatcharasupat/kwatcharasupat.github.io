<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> karn watcharasupat </title> <meta name="author" content="karn watcharasupat"> <meta name="description" content="An (un)sound lab cat. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website, music-informatics, machine-learning, signal-processing, deep-learning "> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%90%B1&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://kwatcharasupat.github.io/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">karn</span> watcharasupat </h1> <p class="desc">an (un)sound lab cat. also known as กานต์ วัชระสุภัทร and 黄株绮 (ng su yi).</p> </header> <article> <div class="profile float-right"> </div> <div class="clearfix"> <p>I am a PhD student in Music Technology at the <a href="https://musicinformatics.gatech.edu/" rel="external nofollow noopener" target="_blank">Music Informatics Group</a>, Georgia Institute of Technology under <a href="https://www.alexanderlerch.com/" rel="external nofollow noopener" target="_blank"> Prof Alexander Lerch</a>. I work basically on anything machine learning and signal processing for audio and music applications, as long as I am not trying to replace human creativity. For now, it looks like I will be focusing on audio source separation for a while.</p> <p>Places I once caused chaos in include: <a href="https://machinelearning.apple.com/" rel="external nofollow noopener" target="_blank">Apple</a>’s Acoustics Machine Learning and Acoustics User Studies Teams, <a href="https://research.netflix.com/" rel="external nofollow noopener" target="_blank">Netflix</a>’s Audio Algorithms Team, <a href="https://dr.ntu.edu.sg/cris/rp/rp00961" rel="external nofollow noopener" target="_blank">Prof Woon-Seng Gan</a>’s Digital Signal Processing Lab and <a href="https://www.ntu.edu.sg/sntl" rel="external nofollow noopener" target="_blank">Smart Nation Translational Lab</a>, <a href="https://aevice.com/" rel="external nofollow noopener" target="_blank">Aevice Health</a>, and <a href="https://personal.ntu.edu.sg/andykhong/index.htm" rel="external nofollow noopener" target="_blank">A/P Andy Khong</a>’s Group.</p> <p>Outside of research, I play the euphonium (and other brass instruments). I compose and arrange for wind band once in a while, although you could imagine that there are a lot of unfinished projects on my past and present laptops.</p> <p>In case you have not noticed, there is a chance that I am actually a cat. That would explain a lot of things…</p> <p>For a formal bio, see <a href="/about_formal">here</a>.</p> </div> <h2> <a href="/news/" style="color: inherit">news</a> </h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">May 18, 2024</th> <td> I am interning with Netflix again this summer! </td> </tr> <tr> <th scope="row" style="width: 20%">Apr 19, 2024</th> <td> My piece “A Prelude to the Cyborg Cistern” was played at the <a href="https://music.gatech.edu/laptop-orchestra-project-studio-04192024" rel="external nofollow noopener" target="_blank">Project Studio Recital</a>. </td> </tr> <tr> <th scope="row" style="width: 20%">Apr 12, 2024</th> <td> Karn will be teleporting to Korea to present my work on <a href="https://ieeexplore.ieee.org/document/10447947" rel="external nofollow noopener" target="_blank">spatial audio</a> and another work with Netflix on <a href="https://ieeexplore.ieee.org/document/10342812" rel="external nofollow noopener" target="_blank">cinematic audio source separation</a>! </td> </tr> <tr> <th scope="row" style="width: 20%">Feb 22, 2024</th> <td> After a 4-year hiatus, I am <a href="https://music.gatech.edu/concert-band-and-symphonic-band-2024-02-22" rel="external nofollow noopener" target="_blank">performing</a> again as euphonium player! </td> </tr> <tr> <th scope="row" style="width: 20%">Oct 02, 2023</th> <td> Our proposal was awarded the <a href="https://research.gatech.edu/ideas-awards-grants-and-cyberinfrastructure-resources-thematic-programs-and-research-ai" rel="external nofollow noopener" target="_blank">IDEaS Cyberinfrastructure Resource Grant!</a> </td> </tr> </table> </div> </div> <h2> <a href="/blog/" style="color: inherit">latest posts</a> </h2> <div class="news"> <div class="table-responsive"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">May 10, 2024</th> <td> <a class="news-title" href="/blog/2024/spauq/">Quantifying Spatial Audio Quality Impairment</a> </td> </tr> <tr> <th scope="row" style="width: 20%">Feb 25, 2022</th> <td> <a class="news-title" href="https://musicinformatics.gatech.edu/project/latte-cross-framework-python-package-for-evaluation-of-latent-based-generative-models/" target="_blank" rel="external nofollow noopener">Latte: Cross-framework Python Package for Evaluation of Latent-Based Generative Models</a> <svg width="2rem" height="2rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </td> </tr> </table> </div> </div> <h2> <a href="/publications/" style="color: inherit">selected publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#00369f"> <div>Journal</div> </abbr> </div> <div id="Watcharasupat2023GeneralizedBandsplitNeural" class="col-sm-8"> <div class="title">A Generalized Bandsplit Neural Network for Cinematic Audio Source Separation</div> <div class="author"> <em>Karn N. Watcharasupat</em>, <a href="https://www.linkedin.com/in/chih-wei-wu-73081689/" rel="external nofollow noopener" target="_blank">Chih-Wei Wu</a>, <a href="https://suncerock.github.io/" rel="external nofollow noopener" target="_blank">Yiwei Ding</a>, <a href="https://github.com/ruohoruotsi" rel="external nofollow noopener" target="_blank">Iroro Orife</a>, Aaron J. Hipple, Phillip A. Williams, Scott Kramer, <a href="https://www.alexanderlerch.com/" rel="external nofollow noopener" target="_blank">Alexander Lerch</a>, and William Wolcott </div> <div class="periodical"> <em>IEEE Open Journal of Signal Processing</em>, Dec 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/10342812" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=10342812" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/karnwatcharasupat/bandit" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="1" data-badge-type="2" data-badge-popover="right" data-doi="10.1109/OJSP.2023.3339428"></span> <span class="__dimensions_badge_embed__" data-doi="10.1109/OJSP.2023.3339428" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=5-5SQ9EAAAAJ&amp;citation_for_view=5-5SQ9EAAAAJ:Wp0gIr-vW9MC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-N/A-4285F4?logo=googlescholar&amp;labelColor=beige" alt="N/A Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p> Cinematic audio source separation is a relatively new subtask of audio source separation, with the aim of extracting the dialogue, music, and effects stems from their mixture. In this work, we developed a model generalizing the Bandsplit RNN for any complete or overcomplete partitions of the frequency axis. Psychoacoustically motivated frequency scales were used to inform the band definitions which are now defined with redundancy for more reliable feature extraction. A loss function motivated by the signal-tonoise ratio and the sparsity-promoting property of the 1-norm was proposed. We additionally exploit the information-sharing property of a common-encoder setup to reduce computational complexity during both training and inference, improve separation performance for hard-to-generalize classes of sounds, and allow flexibility during inference time with detachable decoders. Our best model sets the state of the art on the Divide and Remaster dataset with performance above the ideal ratio mask for the dialogue stem. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Watcharasupat2023GeneralizedBandsplitNeural</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Generalized Bandsplit Neural Network for Cinematic Audio Source Separation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Watcharasupat, Karn N. and Wu, Chih-Wei and Ding, Yiwei and Orife, Iroro and Hipple, Aaron J. and Williams, Phillip A. and Kramer, Scott and Lerch, Alexander and Wolcott, William}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Open Journal of Signal Processing}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{5}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{73--81}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/OJSP.2023.3339428}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2644-1322}</span><span class="p">,</span>
  <span class="na">google_scholar_id</span> <span class="p">=</span> <span class="s">{Wp0gIr-vW9MC}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#00369f"> <div>Journal</div> </abbr> </div> <div id="Watcharasupat2022QuantitativeEvaluationApproach" class="col-sm-8"> <div class="title"> Quantitative Evaluation Approach for Translation of Perceptual Soundscape Attributes: Initial Application to the Thai Language </div> <div class="author"> <em>Karn N. Watcharasupat</em>, Sureenate Jaratjarungkiat, <a href="https://www.linkedin.com/in/lambhan" rel="external nofollow noopener" target="_blank">Bhan Lam</a>, Sujinat Jitwiriyanont, <a href="https://www.linkedin.com/in/kenneth-ooi-31919b39" rel="external nofollow noopener" target="_blank">Kenneth Ooi</a>, Zhen Ting Ong, Nitipong Pichetpan, Kanyanut Akaratham, Titima Suthiwan, Monthita Rojtinnakorn, and <a href="https://www3.ntu.edu.sg/home/ewsgan/" rel="external nofollow noopener" target="_blank">Woon Seng Gan</a> </div> <div class="periodical"> <em>Applied Acoustics</em>, Nov 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2203.12245" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.sciencedirect.com/science/article/abs/pii/S0003682X2200336X" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="1" data-badge-type="2" data-badge-popover="right" data-doi="10.1016/j.apacoust.2022.108962"></span> <span class="__dimensions_badge_embed__" data-doi="10.1016/j.apacoust.2022.108962" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=5-5SQ9EAAAAJ&amp;citation_for_view=5-5SQ9EAAAAJ:Se3iqnhoufwC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-N/A-4285F4?logo=googlescholar&amp;labelColor=beige" alt="N/A Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p> Translation of perceptual soundscape attributes from one language to another remains a challenging task that requires a high degree of fidelity in both psychoacoustic and psycholinguistic senses across the target population. Due to the inherently subjective nature of human perception, translating soundscape attributes using only small focus group discussions or expert panels could lead to translations with psycholinguistic meanings that, in a non-expert setting, deviate or distort from that of the source language. In this work, we present a quantitative evaluation method based on the circumplex model of soundscape perception to assess the overall translation quality. By establishing a set of criteria for evaluating the linguistic and psychometric properties of the translation candidates, statistical analyses can be performed to objectively assess specific strengths and weaknesses of the translation candidates before committing to listening tests or more involved validation experiments. As an initial application domain, we demonstrated the use of the quantitative evaluation framework in the context of an English-to-Thai translation of soundscape attributes. A total of 31 participants who are bilingual in English and Thai were recruited to assess the translation candidates. Subsequent statistical analysis of the evaluation scores revealed acoustico-psycholinguistic properties of the translation candidates which were not previously identified by the expert panel and facilitated a more objective selection of the final translations for subsequent usage. Additionally, with specific biases of the final translations determined numerically, mathematical and statistical techniques for corrections of the survey data may be employed in the future to improve cross-lingual compatibility in soundscape evaluation. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Watcharasupat2022QuantitativeEvaluationApproach</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{
  		Quantitative Evaluation Approach for Translation of Perceptual Soundscape
  		Attributes: Initial Application to the Thai Language
  	}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Watcharasupat, Karn N. and Jaratjarungkiat, Sureenate and Lam, Bhan and Jitwiriyanont, Sujinat and Ooi, Kenneth and Ong, Zhen Ting and Pichetpan, Nitipong and Akaratham, Kanyanut and Suthiwan, Titima and Rojtinnakorn, Monthita and Gan, Woon Seng}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Applied Acoustics}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Elsevier Ltd}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{200}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1016/j.apacoust.2022.108962}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{1872910X}</span><span class="p">,</span>
  <span class="na">google_scholar_id</span> <span class="p">=</span> <span class="s">{Se3iqnhoufwC}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#00369f"> <div>Journal</div> </abbr> </div> <div id="Watcharasupat2022AutonomousInSituSoundscape" class="col-sm-8"> <div class="title"> Autonomous In-Situ Soundscape Augmentation via Joint Selection of Masker and Gain </div> <div class="author"> <em>Karn N. Watcharasupat</em>, <a href="https://www.linkedin.com/in/kenneth-ooi-31919b39" rel="external nofollow noopener" target="_blank">Kenneth Ooi</a>, <a href="https://www.linkedin.com/in/lambhan" rel="external nofollow noopener" target="_blank">Bhan Lam</a>, Trevor Wong, Zhen Ting Ong, and <a href="https://www3.ntu.edu.sg/home/ewsgan/" rel="external nofollow noopener" target="_blank">Woon Seng Gan</a> </div> <div class="periodical"> <em>IEEE Signal Processing Letters</em>, Jul 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2204.13883" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/9841611" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://api.zotero.org/users/7862008/publications/items/HGBUI9DY/file/view" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="1" data-badge-type="2" data-badge-popover="right" data-doi="10.1109/lsp.2022.3194419"></span> <span class="__dimensions_badge_embed__" data-doi="10.1109/lsp.2022.3194419" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=5-5SQ9EAAAAJ&amp;citation_for_view=5-5SQ9EAAAAJ:5nxA0vEk-isC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-N/A-4285F4?logo=googlescholar&amp;labelColor=beige" alt="N/A Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p> The selection of maskers and playback gain levels in an in-situ soundscape augmentation system is crucial to its effectiveness in improving the overall acoustic comfort of a given environment. Traditionally, the selection of appropriate maskers and gain levels has been informed by expert opinion, which may not be representative of the target population, or by listening tests, which can be time- and labor-intensive. Furthermore, the resulting static choices of masker and gain are often inflexible to dynamic real-world soundscapes. In this work, we utilized a deep learning model to perform joint selection of the optimal masker and its gain level for a given soundscape. The proposed model was designed with highly modular building blocks, allowing for an optimized inference process that can quickly search through a large number of masker-gain combinations. In addition, we introduced the use of feature-domain soundscape augmentation conditioned on the digital gain level, eliminating the computationally expensive waveform-domain mixing process during inference, as well as the tedious gain adjustment process required for new maskers. The proposed system was evaluated on a large-scale dataset of subjective responses to augmented soundscapes with 442 participants, with the best model achieving a mean squared error of <inline-formula><tex-math notation="LaTeX">}{0.122}}mathbf {}pm }{0.005}{</tex-math></inline-formula> on pleasantness score, validating the ability of the model to predict combined effect of the masker and its gain level on the perceptual pleasantness level. The proposed system thus allows in-situ or mixed-reality soundscape augmentation to be performed autonomously with near real-time latency while continuously accounting for changes in acoustic environments. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Watcharasupat2022AutonomousInSituSoundscape</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{
  		Autonomous In-Situ Soundscape Augmentation via Joint Selection of Masker and
  		Gain
  	}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Watcharasupat, Karn N. and Ooi, Kenneth and Lam, Bhan and Wong, Trevor and Ong, Zhen Ting and Gan, Woon Seng}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Signal Processing Letters}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{29}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1749--1753}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/lsp.2022.3194419}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{15582361}</span><span class="p">,</span>
  <span class="na">google_scholar_id</span> <span class="p">=</span> <span class="s">{5nxA0vEk-isC}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%6B%77%61%74%63%68%61%72%61%73%75%70%61%74@%67%61%74%65%63%68.%65%64%75" title="email"><i class="fa-solid fa-envelope fa-2xs "></i></a> <a href="https://orcid.org/0000-0002-3878-5048" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="ai ai-orcid fa-2xs "></i></a> <a href="https://scholar.google.com/citations?user=5-5SQ9EAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar fa-2xs "></i></a> <a href="https://www.semanticscholar.org/author/2132046510" title="Semantic Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-semantic-scholar fa-2xs "></i></a> <a href="https://publons.com/a/X-9640-2018/" title="Publons" rel="external nofollow noopener" target="_blank"><i class="ai ai-publons fa-2xs "></i></a> <a href="https://osf.io/vhwa9/" title="Open Science Framework" rel="external nofollow noopener" target="_blank"><i class="ai ai-osf fa-2xs "></i></a> <a href="https://www.researchgate.net/profile/Karn-Watcharasupat-2/" title="ResearchGate" rel="external nofollow noopener" target="_blank"><i class="ai ai-researchgate fa-2xs "></i></a> <a href="https://ieeexplore.ieee.org/author/37088933930/" title="IEEE Xplore" rel="external nofollow noopener" target="_blank"><i class="ai ai-ieee fa-2xs "></i></a> <a href="https://dl.acm.org/profile/99660768673/" title="ACM DL" rel="external nofollow noopener" target="_blank"><i class="ai ai-acm fa-2xs "></i></a> <a href="https://www.scopus.com/authid/detail.uri?authorId=57219750782" title="Scopus" rel="external nofollow noopener" target="_blank"><i class="ai ai-scopus fa-2xs "></i></a> <a href="https://github.com/kwatcharasupat" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github fa-2xs "></i></a> <a href="https://www.linkedin.com/in/kwatcharasupat" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin fa-2xs "></i></a> <a href="https://musicinformatics.gatech.edu" title="Work" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-briefcase fa-2xs "></i></a> <a href="https://dblp.org/pid/268/0507.html" title="DBLP" rel="external nofollow noopener" target="_blank"><i class="ai ai-dblp fa-2xs "></i></a> <a href="https://www.zotero.org/kwatcharasupat" title="Zotero" rel="external nofollow noopener" target="_blank"><i class="ai ai-zotero fa-2xs "></i></a> <a href="/feed.xml" title="RSS Feed"><i class="fa-solid fa-square-rss fa-2xs "></i></a> </div> <div class="contact-note"></div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 karn watcharasupat. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-0CJ2WMKNQY"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-0CJ2WMKNQY");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>