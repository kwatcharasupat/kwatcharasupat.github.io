<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> publications | karn watcharasupat </title> <meta name="author" content="karn watcharasupat"> <meta name="description" content="publications in reversed chronological order. generated by jekyll-scholar. the satp consortium is an international collaboration of 50+ researchers."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website, music-informatics, machine-learning, signal-processing, deep-learning "> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%90%B1&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://kwatcharasupat.github.io/publications/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">karn</span> watcharasupat </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">publications <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">publications</h1> <p class="post-description">publications in reversed chronological order. generated by jekyll-scholar. the satp consortium is an international collaboration of 50+ researchers.</p> </header> <article> <div class="publications"> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#009f06"> <div>Conference</div> </abbr> </div> <div id="Watcharasupat2024StemAgnosticSingleDecoderSystem" class="col-sm-8"> <div class="title">A Stem-Agnostic Single-Decoder System for Music Source Separation Beyond Four Stems</div> <div class="author"> <em>Karn N. Watcharasupat</em>, and <a href="https://www.alexanderlerch.com/" rel="external nofollow noopener" target="_blank">Alexander Lerch</a> </div> <div class="periodical"> <em>In To appear in the Proceedings of the 25th conference of the International Society for Music Information Retrieval</em> , Nov 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2406.18747" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/html/" class="btn btn-sm z-depth-0" role="button">HTML</a> <a href="https://arxiv.org/pdf/2406.18747" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="" class="btn btn-sm z-depth-0" role="button">Blog</a> <a href="https://github.com/karnwatcharasupat/query-bandit" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="1" data-badge-type="2" data-badge-popover="right" data-altmetric-id="true"></span> <span class="__dimensions_badge_embed__" data-id="true" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=5-5SQ9EAAAAJ&amp;citation_for_view=5-5SQ9EAAAAJ:-f6ydRqryjwC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-1-4285F4?logo=googlescholar&amp;labelColor=beige" alt="1 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Despite significant recent progress across multiple subtasks of audio source separation, few music source separation systems support separation beyond the four-stem vocals, drums, bass, and other (VDBO) setup. Of the very few current systems that support source separation beyond this setup, most continue to rely on an inflexible decoder setup that can only support a fixed pre-defined set of stems. Increasing stem support in these inflexible systems correspondingly requires increasing computational complexity, rendering extensions of these systems computationally infeasible for long-tail instruments. In this work, we propose Banquet, a system that allows source separation of multiple stems using just one decoder. A bandsplit source separation model is extended to work in a query-based setup in tandem with a music instrument recognition PaSST model. On the MoisesDB dataset, Banquet, at only 24.9 M trainable parameters, approached the performance level of the significantly more complex 6-stem Hybrid Transformer Demucs on VDBO stems and outperformed it on guitar and piano. The query-based setup allows for the separation of narrow instrument classes such as clean acoustic guitars, and can be successfully applied to the extraction of less common stems such as reeds and organs.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Watcharasupat2024StemAgnosticSingleDecoderSystem</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Stem-Agnostic Single-Decoder System for Music Source Separation Beyond Four Stems}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Watcharasupat, Karn N. and Lerch, Alexander}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{To appear in the Proceedings of the 25th conference of the International Society for Music Information Retrieval}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{ISMIR}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{San Francisco, CA, USA}</span><span class="p">,</span>
  <span class="na">google_scholar_id</span> <span class="p">=</span> <span class="s">{-f6ydRqryjwC}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#009f06"> <div>Conference</div> </abbr> </div> <div id="Watcharasupat2024RemasteringDivideRemaster" class="col-sm-8"> <div class="title">Remastering Divide and Remaster: A Cinematic Audio Source Separation Dataset with Multilingual Support</div> <div class="author"> <em>Karn N. Watcharasupat</em>, <a href="https://www.linkedin.com/in/chih-wei-wu-73081689/" rel="external nofollow noopener" target="_blank">Chih-Wei Wu</a>, and <a href="https://github.com/ruohoruotsi" rel="external nofollow noopener" target="_blank">Iroro Orife</a> </div> <div class="periodical"> <em>In To appear in the Proceedings of the 5th IEEE International Symposium on the Internet of Sounds</em> , Sep 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2406.18747" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/html/" class="btn btn-sm z-depth-0" role="button">HTML</a> <a href="https://arxiv.org/abs/2407.07275" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="" class="btn btn-sm z-depth-0" role="button">Blog</a> <a href="https://github.com/kwatcharasupat/source-separation-landing" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="1" data-badge-type="2" data-badge-popover="right" data-altmetric-id="true"></span> <span class="__dimensions_badge_embed__" data-id="true" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=5-5SQ9EAAAAJ&amp;citation_for_view=5-5SQ9EAAAAJ:mB3voiENLucC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-1-4285F4?logo=googlescholar&amp;labelColor=beige" alt="1 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Cinematic audio source separation (CASS), as a problem of extracting the dialogue, music, and effects stems from their mixture, is a relatively new subtask of audio source separation. To date, only one publicly available dataset exists for CASS, that is, the Divide and Remaster (DnR) dataset, which is currently at version 2. While DnR v2 has been an incredibly useful resource for CASS, several areas of improvement have been identified, particularly through its use in the 2023 Sound Demixing Challenge. In this work, we develop version 3 of the DnR dataset, addressing issues relating to vocal content in non-dialogue stems, loudness distributions, mastering process, and linguistic diversity. In particular, the dialogue stem of DnR v3 includes speech content from more than 30 languages from multiple families including but not limited to the Germanic, Romance, Indo-Aryan, Dravidian, Malayo-Polynesian, and Bantu families. Benchmark results using the Bandit model indicated that training on multilingual data yields significant generalizability to the model even in languages with low data availability. Even in languages with high data availability, the multilingual model often performs on par or better than dedicated models trained on monolingual CASS datasets.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Watcharasupat2024RemasteringDivideRemaster</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Remastering Divide and Remaster: A Cinematic Audio Source Separation Dataset with Multilingual Support}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Watcharasupat, Karn N. and Wu, Chih-Wei and Orife, Iroro}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">sep</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{To appear in the Proceedings of the 5th IEEE International Symposium on the Internet of Sounds}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Erlangen, Germany}</span><span class="p">,</span>
  <span class="na">google_scholar_id</span> <span class="p">=</span> <span class="s">{mB3voiENLucC}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#00369f"> <div>Journal</div> </abbr> </div> <div id="Aletta2024SoundscapeDescriptorsEighteena" class="col-sm-8"> <div class="title">Soundscape Descriptors in Eighteen Languages: Translation and Validation through Listening Experiments</div> <div class="author"> Francesco Aletta, Andrew Mitchell, Tin Oberman, Jian Kang, Sara Khelil, Tallal Abdel Karim Bouzir, Djihed Berkouk, Hui Xie, Yuan Zhang, Ruining Zhang, Xinhao Yang, Min Li, Kristian Jambrošić, Tamara Zaninović, Kirsten van den Bosch, Tamara Lühr, Nicolas Orlik, Darragh Fitzpatrick, Anastasios Sarampalis, Pierre Aumond, Catherine Lavandier, Cleopatra Christina Moshona, Steffen Lepa, André Fiebig, Nikolaos M. Papadakis, Georgios E. Stavroulakis, Anugrah Sabdono Sudarsono, Sugeng Joko Sarwono, Giuseppina Emma Puglisi, Farid Jafari, Arianna Astolfi, Louena Shtrepi, Koji Nagahata, Hyun In Jo, Jin Yong Jeon, <a href="https://www.linkedin.com/in/lambhan" rel="external nofollow noopener" target="_blank">Bhan Lam</a>, Julia Chieng, <a href="https://www.linkedin.com/in/kenneth-ooi-31919b39" rel="external nofollow noopener" target="_blank">Kenneth Ooi</a>, Joo Young Hong, Sónia Monteiro Antunes, Sonia Alves, Maria Luiza de Ulhoa Carvalho, Ranny Loureiro Xavier Nascimento Michalski, Pablo Kogan, Jerónimo Vida Manzano, Rafael García Quesada, Enrique Suárez Silva, José Antonio Almagro Pastor, Mats E. Nilsson, Östen Axelsson, <a href="https://www3.ntu.edu.sg/home/ewsgan/" rel="external nofollow noopener" target="_blank">Woon-Seng Gan</a>, <em>Karn N. Watcharasupat</em>, Sureenate Jaratjarungkiat, Zhen-Ting Ong, Papatya Nur Dökmeci Yörükoğlu, Uğur Beyza Erçakmak Osma , and Thu Lan Nguyen </div> <div class="periodical"> <em>Applied Acoustics</em>, Sep 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.sciencedirect.com/science/article/pii/S0003682X24002603" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="1" data-badge-type="2" data-badge-popover="right" data-doi="10.1016/j.apacoust.2024.110109"></span> <span class="__dimensions_badge_embed__" data-doi="10.1016/j.apacoust.2024.110109" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=5-5SQ9EAAAAJ&amp;citation_for_view=5-5SQ9EAAAAJ:hC7cP41nSMkC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-4-4285F4?logo=googlescholar&amp;labelColor=beige" alt="4 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>This paper presents the outcomes of the “Soundscape Attributes Translation Project” (SATP), an international initiative addressing the critical research gap in soundscape descriptors translations for cross-cultural studies. Focusing on eighteen languages – namely: Arabic, Chinese, Croatian, Dutch, English, French, German, Greek, Indonesian, Italian, Japanese, Korean, Malay, Portuguese, Spanish, Swedish, Turkish, and Vietnamese – the study employs a four-step procedure to evaluate the reliability and cross-cultural validity of translated soundscape descriptors. The study introduces a three-tier confidence level system (Low, Medium, High) based on “adjusted angles”, which are a measure proposed to correct the soundscape circumplex model (i.e., the pleasant-eventful space proposed in the ISO 12913 series) of a given language. Results reveal that most languages successfully maintain the quasi-circumplex structure of the original soundscape model, ensuring robust cross-cultural validity. English, Arabic, Chinese (Mandarin), Croatian, Dutch, German, Greek, Indonesian, Italian, Spanish, Swedish, and Turkish achieve a “High” confidence level. French, Japanese, Korean, Malay, Portuguese, and Vietnamese demonstrate varying confidence levels, highlighting the importance of the preliminary translation. This research significantly contributes to standardized cross-cultural methodologies in soundscape perception research, emphasizing the pivotal role of adjusted angles within the soundscape circumplex model in ensuring the accuracy of dimensions (i.e., attributes) locations. The SATP initiative offers insights into the complex interplay of language and meaning in the perception of environmental sounds, opening avenues for further cross-cultural soundscape research.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Aletta2024SoundscapeDescriptorsEighteena</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Soundscape Descriptors in Eighteen Languages: {{Translation}} and Validation through Listening Experiments}</span><span class="p">,</span>
  <span class="na">shorttitle</span> <span class="p">=</span> <span class="s">{Soundscape Descriptors in Eighteen Languages}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Aletta, Francesco and Mitchell, Andrew and Oberman, Tin and Kang, Jian and Khelil, Sara and Bouzir, Tallal Abdel Karim and Berkouk, Djihed and Xie, Hui and Zhang, Yuan and Zhang, Ruining and Yang, Xinhao and Li, Min and Jambro{\v s}i{\'c}, Kristian and Zaninovi{\'c}, Tamara and {van den Bosch}, Kirsten and L{\"u}hr, Tamara and Orlik, Nicolas and Fitzpatrick, Darragh and Sarampalis, Anastasios and Aumond, Pierre and Lavandier, Catherine and Moshona, Cleopatra Christina and Lepa, Steffen and Fiebig, Andr{\'e} and Papadakis, Nikolaos M. and Stavroulakis, Georgios E. and Sudarsono, Anugrah Sabdono and Sarwono, Sugeng Joko and Puglisi, Giuseppina Emma and Jafari, Farid and Astolfi, Arianna and Shtrepi, Louena and Nagahata, Koji and Jo, Hyun In and Jeon, Jin Yong and Lam, Bhan and Chieng, Julia and Ooi, Kenneth and Hong, Joo Young and Monteiro Antunes, S{\'o}nia and Alves, Sonia and {de Ulhoa Carvalho}, Maria Luiza and Michalski, Ranny Loureiro Xavier Nascimento and Kogan, Pablo and Vida Manzano, Jer{\'o}nimo and Garc{\'i}a Quesada, Rafael and Su{\'a}rez Silva, Enrique and Almagro Pastor, Jos{\'e} Antonio and Nilsson, Mats E. and Axelsson, {\"O}sten and Gan, Woon-Seng and Watcharasupat, Karn N. and Jaratjarungkiat, Sureenate and Ong, Zhen-Ting and D{\"o}kmeci Y{\"o}r{\"u}ko{\u g}lu, Papatya Nur and Er{\c c}akmak Osma, U{\u g}ur Beyza and Nguyen, Thu Lan}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">sep</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Applied Acoustics}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{224}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{110109}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{0003-682X}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1016/j.apacoust.2024.110109}</span><span class="p">,</span>
  <span class="na">google_scholar_id</span> <span class="p">=</span> <span class="s">{hC7cP41nSMkC}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#009f06"> <div>Conference</div> </abbr> </div> <div id="Aletta2024AdvancingCrossculturalSoundscape" class="col-sm-8"> <div class="title"> Advancing Cross-Cultural Soundscape Research: Updates from the Soundscape Attributes Translation Project (SATP) </div> <div class="author"> Francesco Aletta, Tin Oberman, Andrew Mitchell, Jian Kang, and  the SATP Consortium </div> <div class="periodical"> <em>In Proceedings of the 53rd International Congress and Exposition on Noise Control Engineering </em> , Aug 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://api.zotero.org/users/7862008/publications/items/9MVFMS7C/file/view" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="1" data-badge-type="2" data-badge-popover="right" data-altmetric-id="true"></span> <span class="__dimensions_badge_embed__" data-id="true" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=5-5SQ9EAAAAJ&amp;citation_for_view=5-5SQ9EAAAAJ:7PzlFSSx8tAC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-0-4285F4?logo=googlescholar&amp;labelColor=beige" alt="0 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p> The Soundscape Attributes Translation Project (SATP) addresses linguistic barriers in soundscape research, translating ISO/TS 12913-2:2018 descriptors into various languages for global use. This paper presents recent updates from national working groups, including translations in Turkish, French, Indonesian, Albanian, Chinese, Japanese, Vietnamese, Dutch, and Spanish. A preliminary validation of all the current 18 SATP translations assesses cross-cultural robustness. SATP employs diverse methods for translation, fostering international collaboration. The preliminary validation evaluates the reliability and validity of translated descriptors across languages. Most languages maintain the quasi-circumplex structure of the original soundscape model. English, Arabic, Chinese (Mandarin), Croatian, Dutch, German, Greek, Indonesian, Italian, Spanish, Swedish, and Turkish achieve a “High” confidence level. French, Japanese, Korean, Malay, Portuguese, and Vietnamese show varying confidence levels, emphasizing the need for rigorous validation criteria. SATP advances global soundscape research, with updates from national working groups contributing to cross-cultural relevance. Preliminary validation results affirm the quasi-circumplex structure’s maintenance in most languages, emphasizing the project’s commitment to comprehensive and globally applicable soundscape research instruments. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Aletta2024AdvancingCrossculturalSoundscape</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{
  		Advancing Cross-Cultural Soundscape Research: Updates from the Soundscape
  		Attributes Translation Project (SATP)
  	}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Aletta, Francesco and Oberman, Tin and Mitchell, Andrew and Kang, Jian and {the SATP Consortium}}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">aug</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{
  		Proceedings of the 53rd International Congress and Exposition on Noise
  		Control Engineering
  	}</span><span class="p">,</span>
  <span class="na">google_scholar_id</span> <span class="p">=</span> <span class="s">{7PzlFSSx8tAC}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#00369f"> <div>Journal</div> </abbr> </div> <div id="Watcharasupat2024ValidatingThaiTranslations" class="col-sm-8"> <div class="title"> Validating Thai Translations of Perceptual Soundscape Attributes: A Non-Procrustean Approach with a Procrustes Projection </div> <div class="author"> <em>Karn N. Watcharasupat</em>, <a href="https://www.linkedin.com/in/kenneth-ooi-31919b39" rel="external nofollow noopener" target="_blank">Kenneth Ooi</a>, <a href="https://www.linkedin.com/in/lambhan" rel="external nofollow noopener" target="_blank">Bhan Lam</a>, Zhen-Ting Ong, Sureenate Jaratjarungkiat, and <a href="https://www3.ntu.edu.sg/home/ewsgan/" rel="external nofollow noopener" target="_blank">Woon-Seng Gan</a> </div> <div class="periodical"> <em>Applied Acoustics</em>, May 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.sciencedirect.com/science/article/abs/pii/S0003682X24001506" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://api.zotero.org/users/7862008/publications/items/GCYEBGNH/file/view" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/ntudsp/satp-tha-stage2" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="1" data-badge-type="2" data-badge-popover="right" data-doi="10.1016/j.apacoust.2024.109999"></span> <span class="__dimensions_badge_embed__" data-doi="10.1016/j.apacoust.2024.109999" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=5-5SQ9EAAAAJ&amp;citation_for_view=5-5SQ9EAAAAJ:L8Ckcad2t8MC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-1-4285F4?logo=googlescholar&amp;labelColor=beige" alt="1 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p> Measurement of a psychological construct across populations without a common linguistic medium often necessitates the development of multiple translations of the psychometric tool across multiple languages, dialects, or other population-specific variations. In this follow-up (Stage 2) study, a listening test using a shared set of 27 stimuli from the Soundscape Attribute Translation Project (SATP) was conducted with Thai-speaking participants using the set of Thai translations of the eight perceptual affective quality (PAQ) descriptors selected in earlier (Stage 1) work through a structured evaluation questionnaire. Principal component analysis was performed on the listening test data to obtain a rank-two reduction of the responses with maximal explained variance. In order to align the principal component space to the two-dimensional circumplex space, this work presents a simple and numerically stable method based on the orthogonal Procrustes projection, to find the optimal two-dimensional orthogonal transform that aligns the first two principal components with the axes corresponding to Pleasantness and Eventfulness as defined in ISO/TS 12913-3:2019. Analysis of the listening test responses indicated good to excellent interrater reliability, reflecting the general comprehensibility of the translations to laypersons. Subsequent analyses yielded a two-dimensional projection with 94.4 % explained variance and near-perfect alignment of the composite Pleasantness and Eventfulness axes. Angular locations of the individual translated PAQs are located within 16^∘ of their theoretically ideal locations and preserve angular ordering, albeit with imperfect equiangularity. Cross-analysis against the results from Stage 1 showed that the structured evaluation may be partially useful in anticipating potential imperfections of the PAQ translations and their angular locations in Stage 2. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Watcharasupat2024ValidatingThaiTranslations</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{
  		Validating Thai Translations of Perceptual Soundscape Attributes: A
  		Non-Procrustean Approach with a Procrustes Projection
  	}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Watcharasupat, Karn N. and Ooi, Kenneth and Lam, Bhan and Ong, Zhen-Ting and Jaratjarungkiat, Sureenate and Gan, Woon-Seng}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Applied Acoustics}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1016/j.apacoust.2024.109999}</span><span class="p">,</span>
  <span class="na">google_scholar_id</span> <span class="p">=</span> <span class="s">{L8Ckcad2t8MC}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#009f06"> <div>Conference</div> </abbr> </div> <div id="Watcharasupat2024QuantifyingSpatialAudio" class="col-sm-8"> <div class="title">Quantifying Spatial Audio Quality Impairment</div> <div class="author"> <em>Karn N. Watcharasupat</em>, and <a href="https://www.alexanderlerch.com/" rel="external nofollow noopener" target="_blank">Alexander Lerch</a> </div> <div class="periodical"> <em>In Proceedings of the 2024 IEEE International Conference on Acoustics, Speech, and Signal Processing </em> , Apr 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/10447947" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://api.zotero.org/users/7862008/publications/items/UJ85MFYJ/file/view" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://kwatcharasupat.github.io/blog/2024/spauq/" class="btn btn-sm z-depth-0" role="button">Blog</a> <a href="https://github.com/karnwatcharasupat/spauq" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="1" data-badge-type="2" data-badge-popover="right" data-doi="10.1109/ICASSP48485.2024.10447947"></span> <span class="__dimensions_badge_embed__" data-doi="10.1109/ICASSP48485.2024.10447947" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=5-5SQ9EAAAAJ&amp;citation_for_view=5-5SQ9EAAAAJ:_kc_bZDykSQC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-1-4285F4?logo=googlescholar&amp;labelColor=beige" alt="1 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p> Spatial audio quality is a highly multifaceted concept, with many interactions between environmental, geometrical, anatomical, psychological, and contextual factors. Methods for characterization or evaluation of the geometrical components of spatial audio quality, however, remain scarce, despite being perhaps the least subjective aspect of spatial audio quality to quantify. By considering interchannel time and level differences relative to a reference signal, it is possible to construct a signal model to isolate some of the spatial distortion. By using a combination of least-square optimization and heuristics, we propose a signal decomposition method to isolate the spatial error, in terms of interchannel gain leakages and changes in relative delays, from a processed signal. This allows the computation of simple energy-ratio metrics, providing objective measures of spatial and non-spatial signal qualities, with minimal assumptions and no dataset dependency. Experiments demonstrate the robustness of the method against common spatial signal degradation introduced by, e.g., audio compression and music source separation. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Watcharasupat2024QuantifyingSpatialAudio</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Quantifying Spatial Audio Quality Impairment}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Watcharasupat, Karn N. and Lerch, Alexander}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">apr</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{
  		Proceedings of the 2024 IEEE International Conference on Acoustics, Speech, and
  		Signal Processing
  	}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Seoul, Korea, Republic of}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{746--750}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ICASSP48485.2024.10447947}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9798350344851}</span><span class="p">,</span>
  <span class="na">google_scholar_id</span> <span class="p">=</span> <span class="s">{_kc_bZDykSQC}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#00369f"> <div>Journal</div> </abbr> </div> <div id="Ooi2024LionCitySoundscapes" class="col-sm-8"> <div class="title"> Lion City Soundscapes: Modified Partitioning around Medoids for a Perceptually Diverse Dataset of Singaporean Soundscapes </div> <div class="author"> <a href="https://www.linkedin.com/in/kenneth-ooi-31919b39" rel="external nofollow noopener" target="_blank">Kenneth Ooi</a>, Jessie Goh, Hao-Weng Lin, Zhen-Ting Ong, Trevor Wong, <em>Karn N. Watcharasupat</em>, <a href="https://www.linkedin.com/in/lambhan" rel="external nofollow noopener" target="_blank">Bhan Lam</a>, and <a href="https://www3.ntu.edu.sg/home/ewsgan/" rel="external nofollow noopener" target="_blank">Woon-Seng Gan</a> </div> <div class="periodical"> <em>JASA Express Letters</em>, Apr 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://doi.org/10.21979/N9/AVHSBX" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://api.zotero.org/users/7862008/publications/items/4FAZKA82/file/view" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://researchdata.ntu.edu.sg/dataset.xhtml?persistentId=doi:10.21979/N9/AVHSBX%0A%09" class="btn btn-sm z-depth-0" role="button">Supp</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="1" data-badge-type="2" data-badge-popover="right" data-doi="10.1121/10.0025830"></span> <span class="__dimensions_badge_embed__" data-doi="10.1121/10.0025830" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=5-5SQ9EAAAAJ&amp;citation_for_view=5-5SQ9EAAAAJ:ZeXyd9-uunAC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-0-4285F4?logo=googlescholar&amp;labelColor=beige" alt="0 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p> This study presents a dataset of audio-visual soundscape recordings at 62 different locations in Singapore, initially made as full-length recordings over spans of 9–38 min. For consistency and reduction in listener fatigue in future subjective studies, one-minute excerpts were cropped from the full-length recordings. An automated method using pre-trained models for Pleasantness and Eventfulness (according to ISO 12913) in a modified partitioning around medoids algorithm was employed to generate the set of excerpts by balancing the need to encompass the perceptual space with uniformity in distribution. A validation study on the method confirmed its adherence to the intended design.. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Ooi2024LionCitySoundscapes</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{
  		Lion City Soundscapes: Modified Partitioning around Medoids for a
  		Perceptually Diverse Dataset of Singaporean Soundscapes
  	}</span><span class="p">,</span>
  <span class="na">shorttitle</span> <span class="p">=</span> <span class="s">{Lion City Soundscapes}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ooi, Kenneth and Goh, Jessie and Lin, Hao-Weng and Ong, Zhen-Ting and Wong, Trevor and Watcharasupat, Karn N. and Lam, Bhan and Gan, Woon-Seng}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">apr</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{JASA Express Letters}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{4}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{4}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{047402}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1121/10.0025830}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2691-1191}</span><span class="p">,</span>
  		<span class="na">https://researchdata.ntu.edu.sg/dataset.xhtml?persistentId</span><span class="p">=</span><span class="nv">doi:10.21979/N9/AVHSBX</span>
  	<span class="p">}</span><span class="c">,</span>
  <span class="c">google_scholar_id = {ZeXyd9-uunAC}</span>
<span class="c">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#00369f"> <div>Journal</div> </abbr> </div> <div id="Ooi2024ARAUSLargeScaleDataset" class="col-sm-8"> <div class="title"> ARAUS: A Large-Scale Dataset and Baseline Models of Affective Responses to Augmented Urban Soundscapes </div> <div class="author"> <a href="https://www.linkedin.com/in/kenneth-ooi-31919b39" rel="external nofollow noopener" target="_blank">Kenneth Ooi</a>, Zhen-Ting Ong, <em>Karn N. Watcharasupat</em>, <a href="https://www.linkedin.com/in/lambhan" rel="external nofollow noopener" target="_blank">Bhan Lam</a>, Joo Young Hong, and <a href="https://www3.ntu.edu.sg/home/ewsgan/" rel="external nofollow noopener" target="_blank">Woon-Seng Gan</a> </div> <div class="periodical"> <em>IEEE Transactions on Affective Computing</em>, Feb 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/10050114" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="1" data-badge-type="2" data-badge-popover="right" data-doi="10.1109/TAFFC.2023.3247914"></span> <span class="__dimensions_badge_embed__" data-doi="10.1109/TAFFC.2023.3247914" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=5-5SQ9EAAAAJ&amp;citation_for_view=5-5SQ9EAAAAJ:kNdYIx-mwKoC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-25-4285F4?logo=googlescholar&amp;labelColor=beige" alt="25 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p> Choosing optimal maskers for existing soundscapes to effect a desired perceptual change via soundscape augmentation is non-trivial due to extensive varieties of maskers and a dearth of benchmark datasets with which to compare and develop soundscape augmentation models. To address this problem, we make publicly available the ARAUS (Affective Responses to Augmented Urban Soundscapes) dataset, which comprises a five-fold cross-validation set and independent test set totaling 25,440 unique subjective perceptual responses to augmented soundscapes presented as audio-visual stimuli. Each augmented soundscape is made by digitally adding “maskers” (bird, water, wind, traffic, construction, or silence) to urban soundscape recordings at fixed soundscape-to-masker ratios. Responses were then collected by asking participants to rate how pleasant, annoying, eventful, uneventful, vibrant, monotonous, chaotic, calm, and appropriate each augmented soundscape was, in accordance with ISO/TS 12913-2:2018. Participants also provided relevant demographic information and completed standard psychological questionnaires. We perform exploratory and statistical analysis of the responses obtained to verify internal consistency and agreement with known results in the literature. Finally, we demonstrate the benchmarking capability of the dataset by training and comparing four baseline models for urban soundscape pleasantness: a low-parameter regression model, a high-parameter convolutional neural network, and two attention-based networks in the literature. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Ooi2024ARAUSLargeScaleDataset</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{
  		ARAUS: A Large-Scale Dataset and Baseline Models of Affective Responses to
  		Augmented Urban Soundscapes
  	}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ooi, Kenneth and Ong, Zhen-Ting and Watcharasupat, Karn N. and Lam, Bhan and Hong, Joo Young and Gan, Woon-Seng}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">feb</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Affective Computing}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{15}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{105--120}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/TAFFC.2023.3247914}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{1949-3045}</span><span class="p">,</span>
  <span class="na">google_scholar_id</span> <span class="p">=</span> <span class="s">{kNdYIx-mwKoC}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#00369f"> <div>Journal</div> </abbr> </div> <div id="Lam2024AutomatingUrbanSoundscape" class="col-sm-8"> <div class="title">Automating urban soundscape enhancements with AI: In-situ assessment of quality and restorativeness in traffic-exposed residential areas</div> <div class="author"> <a href="https://www.linkedin.com/in/lambhan" rel="external nofollow noopener" target="_blank">Bhan Lam</a>, Zhen-Ting Ong, <a href="https://www.linkedin.com/in/kenneth-ooi-31919b39" rel="external nofollow noopener" target="_blank">Kenneth Ooi</a>, Wen-Hui Ong, Trevor Wong, <em>Karn N. Watcharasupat</em>, Vanessa Boey, Irene Lee, Joo Young Hong, Jian Kang, Kar Fye Alvin Lee, Georgios Christopoulos, and <a href="https://www3.ntu.edu.sg/home/ewsgan/" rel="external nofollow noopener" target="_blank">Woon-Seng Gan</a> </div> <div class="periodical"> <em>Building and Environment</em>, Feb 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2407.05744" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.sciencedirect.com/science/article/pii/S036013232400948X" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://arxiv.org/abs/2407.07275" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="" class="btn btn-sm z-depth-0" role="button">Blog</a> <a href="" class="btn btn-sm z-depth-0" role="button">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="1" data-badge-type="2" data-badge-popover="right" data-doi="https://doi.org/10.1016/j.buildenv.2024.112106"></span> <span class="__dimensions_badge_embed__" data-doi="https://doi.org/10.1016/j.buildenv.2024.112106" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=5-5SQ9EAAAAJ&amp;citation_for_view=5-5SQ9EAAAAJ:hFOr9nPyWt4C" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-0-4285F4?logo=googlescholar&amp;labelColor=beige" alt="0 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Formalized in ISO 12913, the “soundscape” approach is a paradigmatic shift towards perception-based urban sound management, aiming to alleviate the substantial socioeconomic costs of noise pollution to advance the United Nations Sustainable Development Goals. Focusing on traffic-exposed outdoor residential sites, we implemented an automatic masker selection system (AMSS) utilizing natural sounds to mask (or augment) traffic soundscapes. We employed a pre-trained AI model to automatically select the optimal masker and adjust its playback level, adapting to changes over time in the ambient environment to maximize “Pleasantness”, a perceptual dimension of soundscape quality in ISO 12913. Our validation study involving (N=68) residents revealed a significant 14.6% enhancement in “Pleasantness” after intervention, correlating with increased restorativeness and positive affect. Perceptual enhancements at the traffic-exposed site matched those at a quieter control site with 6dB(A) lower LA,eq and road traffic noise dominance, affirming the efficacy of AMSS as a soundscape intervention, while streamlining the labour-intensive assessment of “Pleasantness” with probabilistic AI prediction.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Lam2024AutomatingUrbanSoundscape</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Automating urban soundscape enhancements with AI: In-situ assessment of quality and restorativeness in traffic-exposed residential areas}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Building and Environment}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{266}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{112106}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{0360-1323}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1016/j.buildenv.2024.112106}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://www.sciencedirect.com/science/article/pii/S036013232400948X}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lam, Bhan and Ong, Zhen-Ting and Ooi, Kenneth and Ong, Wen-Hui and Wong, Trevor and Watcharasupat, Karn N. and Boey, Vanessa and Lee, Irene and Hong, Joo Young and Kang, Jian and Lee, Kar Fye Alvin and Christopoulos, Georgios and Gan, Woon-Seng}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Urban soundscape, Natural sounds, Auditory masking, Probabilistic approach, Soundscape augmentation, Artificial intelligence}</span><span class="p">,</span>
  <span class="na">google_scholar_id</span> <span class="p">=</span> <span class="s">{hFOr9nPyWt4C}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#00369f"> <div>Journal</div> </abbr> </div> <div id="Watcharasupat2023GeneralizedBandsplitNeural" class="col-sm-8"> <div class="title">A Generalized Bandsplit Neural Network for Cinematic Audio Source Separation</div> <div class="author"> <em>Karn N. Watcharasupat</em>, <a href="https://www.linkedin.com/in/chih-wei-wu-73081689/" rel="external nofollow noopener" target="_blank">Chih-Wei Wu</a>, <a href="https://suncerock.github.io/" rel="external nofollow noopener" target="_blank">Yiwei Ding</a>, <a href="https://github.com/ruohoruotsi" rel="external nofollow noopener" target="_blank">Iroro Orife</a>, Aaron J. Hipple, Phillip A. Williams, Scott Kramer, <a href="https://www.alexanderlerch.com/" rel="external nofollow noopener" target="_blank">Alexander Lerch</a>, and William Wolcott </div> <div class="periodical"> <em>IEEE Open Journal of Signal Processing</em>, Dec 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/10342812" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=10342812" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/karnwatcharasupat/bandit" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="1" data-badge-type="2" data-badge-popover="right" data-doi="10.1109/OJSP.2023.3339428"></span> <span class="__dimensions_badge_embed__" data-doi="10.1109/OJSP.2023.3339428" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=5-5SQ9EAAAAJ&amp;citation_for_view=5-5SQ9EAAAAJ:Wp0gIr-vW9MC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-5-4285F4?logo=googlescholar&amp;labelColor=beige" alt="5 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p> Cinematic audio source separation is a relatively new subtask of audio source separation, with the aim of extracting the dialogue, music, and effects stems from their mixture. In this work, we developed a model generalizing the Bandsplit RNN for any complete or overcomplete partitions of the frequency axis. Psychoacoustically motivated frequency scales were used to inform the band definitions which are now defined with redundancy for more reliable feature extraction. A loss function motivated by the signal-tonoise ratio and the sparsity-promoting property of the 1-norm was proposed. We additionally exploit the information-sharing property of a common-encoder setup to reduce computational complexity during both training and inference, improve separation performance for hard-to-generalize classes of sounds, and allow flexibility during inference time with detachable decoders. Our best model sets the state of the art on the Divide and Remaster dataset with performance above the ideal ratio mask for the dialogue stem. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Watcharasupat2023GeneralizedBandsplitNeural</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Generalized Bandsplit Neural Network for Cinematic Audio Source Separation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Watcharasupat, Karn N. and Wu, Chih-Wei and Ding, Yiwei and Orife, Iroro and Hipple, Aaron J. and Williams, Phillip A. and Kramer, Scott and Lerch, Alexander and Wolcott, William}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Open Journal of Signal Processing}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{5}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{73--81}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/OJSP.2023.3339428}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2644-1322}</span><span class="p">,</span>
  <span class="na">google_scholar_id</span> <span class="p">=</span> <span class="s">{Wp0gIr-vW9MC}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#00369f"> <div>Journal</div> </abbr> </div> <div id="Lam2023CrossingLinguisticCauseway" class="col-sm-8"> <div class="title"> Crossing the Linguistic Causeway: Ethnonational Differences on Soundscape Attributes in Bahasa Melayu </div> <div class="author"> <a href="https://www.linkedin.com/in/lambhan" rel="external nofollow noopener" target="_blank">Bhan Lam</a>, Julia Chieng, <a href="https://www.linkedin.com/in/kenneth-ooi-31919b39" rel="external nofollow noopener" target="_blank">Kenneth Ooi</a>, Zhen Ting Ong, <em>Karn N. Watcharasupat</em>, Joo Young Hong, and <a href="https://www3.ntu.edu.sg/home/ewsgan/" rel="external nofollow noopener" target="_blank">Woon Seng Gan</a> </div> <div class="periodical"> <em>Applied Acoustics</em>, Nov 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2307.03647" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="1" data-badge-type="2" data-badge-popover="right" data-doi="10.1016/j.apacoust.2023.109675"></span> <span class="__dimensions_badge_embed__" data-doi="10.1016/j.apacoust.2023.109675" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=5-5SQ9EAAAAJ&amp;citation_for_view=5-5SQ9EAAAAJ:4TOpqqG69KYC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-7-4285F4?logo=googlescholar&amp;labelColor=beige" alt="7 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p> Despite being neighbouring countries and sharing the language of Bahasa Melayu (ISO 639-3: [Formula presented]), cultural and language education policy differences between Singapore and Malaysia led to differences in the translation of the “annoying” perceived affective quality (PAQ) attribute from English (ISO 639-3: [Formula presented]) to [Formula presented]. This study expands upon the translation of the PAQ attributes from [Formula presented] to [Formula presented] in Stage 1 of the Soundscapes Attributes Translation Project (SATP) initiative, and presents the findings of Stage 2 listening tests that investigated ethnonational differences in the translated [Formula presented] PAQ attributes and explored their circumplexity. A cross-cultural listening test was conducted with 100 [Formula presented] speakers from Malaysia and Singapore using the common SATP protocol. The analysis revealed that Malaysian participants from non-native ethnicities ([Formula presented]) showed PAQ perceptions more similar to Singapore ([Formula presented]) participants than native ethnic Malays ([Formula presented]) in Malaysia. Differences between Singapore and Malaysian groups were primarily observed in stimuli related to water features, reflecting cultural and geographical variations. Besides variations in water source-dominant stimuli perception, disparities between [Formula presented] and [Formula presented] could be mainly attributed to [Formula presented] scores. The findings also suggest that the adoption of region-specific translations, such as [Formula presented] in Singapore and [Formula presented] in Malaysia, adequately addressed differences in the [Formula presented] attribute, since significant differences were observed in one or fewer stimuli across ethnonational groups. The circumplexity analysis indicated that the quasi-circumplex model better fit the data compared to the assumed equal angle quasi-circumplex model in ISO/TS 12913-3, although deviations were observed possibly due to respondents’ unfamiliarity with the United Kingdom-centric context of the stimulus dataset. Furthermore, the alignment between Stage 2 listening tests and quantitative evaluation of attributes in Stage 1 revealed biases in the [Formula presented]–[Formula presented] dimension across ethnonational groups. This study provides insights into the perception of PAQ attributes in cross-cultural and cross-national contexts, facilitating the culturally appropriate adoption of translated PAQ attributes in soundscape evaluation. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Lam2023CrossingLinguisticCauseway</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{
  		Crossing the Linguistic Causeway: Ethnonational Differences on Soundscape
  		Attributes in Bahasa Melayu
  	}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lam, Bhan and Chieng, Julia and Ooi, Kenneth and Ong, Zhen Ting and Watcharasupat, Karn N. and Hong, Joo Young and Gan, Woon Seng}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Applied Acoustics}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Elsevier Ltd}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{214}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1016/j.apacoust.2023.109675}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{1872910X}</span><span class="p">,</span>
  <span class="na">google_scholar_id</span> <span class="p">=</span> <span class="s">{4TOpqqG69KYC}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#009f06"> <div>Conference</div> </abbr> </div> <div id="Aletta2023PreliminaryResultsSoundscape" class="col-sm-8"> <div class="title"> Preliminary Results of the Soundscape Attributes Translation Project (SATP): Lessons Learned and next Steps </div> <div class="author"> Francesco Aletta, Tin Oberman, Andrew Mitchell, Jian Kang, and  the SATP Consortium </div> <div class="periodical"> <em>In Proceedings of the 10th Convention of the European Acoustics Association Forum Acusticum 2023 </em> , Sep 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.61782/fa.2023.0095" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=5-5SQ9EAAAAJ&amp;citation_for_view=5-5SQ9EAAAAJ:dhFuZR0502QC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-11-4285F4?logo=googlescholar&amp;labelColor=beige" alt="11 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p> The ISO/TS 12913-2:2018 document for soundscape data collection provides a questionnaire instrument for researchers and practitioners to use worldwide, but its applicability has been questioned, since it’s only available in English. To address the lack of research on translations of the soundscape descriptors proposed in Method A of the ISO technical specifications (i.e., vibrant, pleasant, calm, uneventful, monotonous, annoying, chaotic, eventful), an international collaboration, the Soundscape Attributes Translation Project (SATP), was initiated to translate the descriptors into several languages, using different methodological approaches, with the goal of validating the translations using standardized listening experiments. This paper presents the current state of advancement of the project, reporting on preliminary results from selected national working groups within the SATP network, as well as discussing the proposed analysis framework to validate the translations. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Aletta2023PreliminaryResultsSoundscape</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{
  		Preliminary Results of the Soundscape Attributes Translation Project (SATP):
  		Lessons Learned and next Steps
  	}</span><span class="p">,</span>
  <span class="na">shorttitle</span> <span class="p">=</span> <span class="s">{
  		Preliminary Results of the {{Soundscape Attributes Translation Project}}
  		({{SATP}})
  	}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Aletta, Francesco and Oberman, Tin and Mitchell, Andrew and Kang, Jian and {the SATP Consortium}}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">sep</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{
  		Proceedings of the 10th Convention of the European Acoustics Association
  		Forum Acusticum 2023
  	}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{European Acoustics Association}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Turin, Italy}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{701--705}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.61782/fa.2023.0095}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{978-88-88942-67-4}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
  <span class="na">dimensions</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
  <span class="na">google_scholar_id</span> <span class="p">=</span> <span class="s">{dhFuZR0502QC}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#009f06"> <div>Conference</div> </abbr> </div> <div id="Lam2023PreliminaryInvestigationShortterm" class="col-sm-8"> <div class="title"> Preliminary Investigation of the Short-Term in Situ Performance of an Automatic Masker Selection System </div> <div class="author"> <a href="https://www.linkedin.com/in/lambhan" rel="external nofollow noopener" target="_blank">Bhan Lam</a>, <a href="https://www.linkedin.com/in/kenneth-ooi-31919b39" rel="external nofollow noopener" target="_blank">Kenneth Ooi</a>, Zhen-Ting Ong, Trevor Wong, <a href="https://www3.ntu.edu.sg/home/ewsgan/" rel="external nofollow noopener" target="_blank">Woon-Seng Gan</a>, and <em>Karn Watcharasupat</em> </div> <div class="periodical"> <em>In Proceedings of the 52nd International Congress and Exposition on Noise Control Engineering </em> , Aug 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="1" data-badge-type="2" data-badge-popover="right" data-doi="10.3397/in_2023_0805"></span> <span class="__dimensions_badge_embed__" data-doi="10.3397/in_2023_0805" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=5-5SQ9EAAAAJ&amp;citation_for_view=5-5SQ9EAAAAJ:4DMP91E08xMC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-1-4285F4?logo=googlescholar&amp;labelColor=beige" alt="1 Google Scholar citations"> </a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Lam2023PreliminaryInvestigationShortterm</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{
  		Preliminary Investigation of the Short-Term in Situ Performance of an
  		Automatic Masker Selection System
  	}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lam, Bhan and Ooi, Kenneth and Ong, Zhen-Ting and Wong, Trevor and Gan, Woon-Seng and Watcharasupat, Karn}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">aug</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{
  		Proceedings of the 52nd International Congress and Exposition on Noise
  		Control Engineering
  	}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.3397/in_2023_0805}</span><span class="p">,</span>
  <span class="na">google_scholar_id</span> <span class="p">=</span> <span class="s">{4DMP91E08xMC}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#009f06"> <div>Conference</div> </abbr> </div> <div id="Ong2023EffectMaskerSelection" class="col-sm-8"> <div class="title"> Effect of Masker Selection Schemes on the Perceived Affective Quality of Soundscapes: A Pilot Study </div> <div class="author"> Zhen-Ting Ong, <a href="https://www.linkedin.com/in/kenneth-ooi-31919b39" rel="external nofollow noopener" target="_blank">Kenneth Ooi</a>, Trevor Wong, <a href="https://www.linkedin.com/in/lambhan" rel="external nofollow noopener" target="_blank">Bhan Lam</a>, <a href="https://www3.ntu.edu.sg/home/ewsgan/" rel="external nofollow noopener" target="_blank">Woon-Seng Gan</a>, and <em>Karn N. Watcharasupat</em> </div> <div class="periodical"> <em>In Proceedings of the 52nd International Congress and Exposition on Noise Control Engineering </em> , Aug 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="1" data-badge-type="2" data-badge-popover="right" data-doi="10.3397/in_2023_0791"></span> <span class="__dimensions_badge_embed__" data-doi="10.3397/in_2023_0791" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=5-5SQ9EAAAAJ&amp;citation_for_view=5-5SQ9EAAAAJ:mVmsd5A6BfQC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-0-4285F4?logo=googlescholar&amp;labelColor=beige" alt="0 Google Scholar citations"> </a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Ong2023EffectMaskerSelection</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{
  		Effect of Masker Selection Schemes on the Perceived Affective Quality of
  		Soundscapes: A Pilot Study
  	}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ong, Zhen-Ting and Ooi, Kenneth and Wong, Trevor and Lam, Bhan and Gan, Woon-Seng and Watcharasupat, Karn N.}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">aug</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{
  		Proceedings of the 52nd International Congress and Exposition on Noise
  		Control Engineering
  	}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.3397/in_2023_0791}</span><span class="p">,</span>
  <span class="na">google_scholar_id</span> <span class="p">=</span> <span class="s">{mVmsd5A6BfQC}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#009f06"> <div>Conference</div> </abbr> </div> <div id="Ooi2023ARAUSv2ExpandedDataset" class="col-sm-8"> <div class="title"> ARAUSv2: An Expanded Dataset and Multimodal Models of Affective Responses to Augmented Urban Soundscapes </div> <div class="author"> <a href="https://www.linkedin.com/in/kenneth-ooi-31919b39" rel="external nofollow noopener" target="_blank">Kenneth Ooi</a>, Zhen-Ting Ong, <a href="https://www.linkedin.com/in/lambhan" rel="external nofollow noopener" target="_blank">Bhan Lam</a>, Trevor Wong, <a href="https://www3.ntu.edu.sg/home/ewsgan/" rel="external nofollow noopener" target="_blank">Woon-Seng Gan</a>, and <em>Karn Watcharasupat</em> </div> <div class="periodical"> <em>In Proceedings of the 52nd International Congress and Exposition on Noise Control Engineering </em> , Aug 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="1" data-badge-type="2" data-badge-popover="right" data-doi="10.3397/in_2023_0459"></span> <span class="__dimensions_badge_embed__" data-doi="10.3397/in_2023_0459" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=5-5SQ9EAAAAJ&amp;citation_for_view=5-5SQ9EAAAAJ:9ZlFYXVOiuMC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-0-4285F4?logo=googlescholar&amp;labelColor=beige" alt="0 Google Scholar citations"> </a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Ooi2023ARAUSv2ExpandedDataset</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{
  		ARAUSv2: An Expanded Dataset and Multimodal Models of Affective Responses to
  		Augmented Urban Soundscapes
  	}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ooi, Kenneth and Ong, Zhen-Ting and Lam, Bhan and Wong, Trevor and Gan, Woon-Seng and Watcharasupat, Karn}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">aug</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{
  		Proceedings of the 52nd International Congress and Exposition on Noise
  		Control Engineering
  	}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.3397/in_2023_0459}</span><span class="p">,</span>
  <span class="na">google_scholar_id</span> <span class="p">=</span> <span class="s">{9ZlFYXVOiuMC}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#b509ac"> <div>Patent</div> </abbr> </div> <div id="Nguyen2021MethodAudioProcessingTW" class="col-sm-8"> <div class="title">Method and audio processing system for blind source separation without sampling rate mismatch estimation (不須計算取樣頻率誤差的盲源分離方法以及音訊處理系統)</div> <div class="author"> Hai Trieu Anh Nguyen (阮海潮英), Wai Hoong Khong (鄺偉雄), Karn Watcharasupat (瓦特察拉蘇帕特 甘), and Qing Liu (劉晴) </div> <div class="periodical"> Taiwan Patent TWI809390B, Granted Jul 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://patents.google.com/patent/TWI809390B" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="1" data-badge-type="2" data-badge-popover="right" data-altmetric-id="true"></span> <span class="__dimensions_badge_embed__" data-id="true" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=5-5SQ9EAAAAJ&amp;citation_for_view=5-5SQ9EAAAAJ:IWHjjKOFINEC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-0-4285F4?logo=googlescholar&amp;labelColor=beige" alt="0 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>A method for blind source separation for an audio processing system including multiple devices is provided. Each of the devices includes multiple microphones. A measure of dissimilarity between the signal vector sensed by each device and a column of a mixing matrix is computed. The measure of dissimilarity is used to establish an objective function and an optimization algorithm is performed to compute the mixing matrix. Estimates of the original signals are computed according to the mixing matrix and the signal vector without estimating a sampling rate mismatch between the devices. Therefore, compensation of sampling rate mismatch is not required. 本揭露提出一種盲源分離方法，適用於一音訊處理系統，此音訊處理系統包括多個裝置，每一個裝置包括多個麥克風。先計算每個裝置感測的訊號向量與混和矩陣的一行之間的差異，此差異用來建立一目標函數，接著執行一最佳化演算法來計算混和矩陣。根據混和矩陣與訊號向量可以計算出原始訊號而不用計算裝置之間的取樣頻率誤差。如此一來，便不需要補償取樣頻率誤差。 </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@patent</span><span class="p">{</span><span class="nl">Nguyen2021MethodAudioProcessingTW</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Nguyen (阮海潮英), Hai Trieu Anh and Khong (鄺偉雄), Wai Hoong and Watcharasupat (瓦特察拉蘇帕特 甘), Karn and Liu (劉晴), Qing}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Method and audio processing system for blind source separation without sampling rate mismatch estimation (不須計算取樣頻率誤差的盲源分離方法以及音訊處理系統)}</span><span class="p">,</span>
  <span class="na">nationality</span> <span class="p">=</span> <span class="s">{Taiwan}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{TWI809390B}</span><span class="p">,</span>
  <span class="na">dayfiled</span> <span class="p">=</span> <span class="s">{13}</span><span class="p">,</span>
  <span class="na">monthfiled</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">yearfiled</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">day</span> <span class="p">=</span> <span class="s">{21}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">granted</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
  <span class="na">google_scholar_id</span> <span class="p">=</span> <span class="s">{IWHjjKOFINEC}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#b509ac"> <div>Patent</div> </abbr> </div> <div id="Nguyen2021MethodAudioProcessingSG" class="col-sm-8"> <div class="title">Method and audio processing system for blind source separation without sampling rate mismatch estimation and compensation</div> <div class="author"> Hai Trieu Anh Nguyen , Andy W. H. Khong, <em>Karn Watcharasupat</em>, and Qing Liu </div> <div class="periodical"> Singapore Patent SG10202102050T, Granted Jul 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://patentscope.wipo.int/search/en/detail.jsf?docId=SG408641786" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="1" data-badge-type="2" data-badge-popover="right" data-altmetric-id="true"></span> <span class="__dimensions_badge_embed__" data-id="true" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=5-5SQ9EAAAAJ&amp;citation_for_view=5-5SQ9EAAAAJ:qxL8FJ1GzNcC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-0-4285F4?logo=googlescholar&amp;labelColor=beige" alt="0 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>A method for blind source separation for an audio processing system including multiple devices is provided. Each of the devices includes multiple microphones. A measure of dissimilarity between the signal vector sensed by each device and a column of a mixing matrix is computed. The measure of dissimilarity is used to establish an objective function and an optimization algorithm is performed to compute the mixing matrix. Estimates of the original signals are computed according to the mixing matrix and the signal vector without estimating a sampling rate mismatch between the devices. Therefore, compensation of sampling rate mismatch is not required.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@patent</span><span class="p">{</span><span class="nl">Nguyen2021MethodAudioProcessingSG</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Nguyen, Hai Trieu Anh and Khong, Andy W. H. and Watcharasupat, Karn and Liu, Qing}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Method and audio processing system for blind source separation without sampling rate mismatch estimation and compensation}</span><span class="p">,</span>
  <span class="na">nationality</span> <span class="p">=</span> <span class="s">{Singapore}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{SG10202102050T}</span><span class="p">,</span>
  <span class="na">dayfiled</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">monthfiled</span> <span class="p">=</span> <span class="nv">mar</span><span class="p">,</span>
  <span class="na">yearfiled</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">day</span> <span class="p">=</span> <span class="s">{24}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">granted</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
  <span class="na">google_scholar_id</span> <span class="p">=</span> <span class="s">{qxL8FJ1GzNcC}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#009f06"> <div>Conference</div> </abbr> </div> <div id="Ooi2023AutonomousSoundscapeAugmentation" class="col-sm-8"> <div class="title"> Autonomous Soundscape Augmentation with Multimodal Fusion of Visual and Participant-linked Inputs </div> <div class="author"> <a href="https://www.linkedin.com/in/kenneth-ooi-31919b39" rel="external nofollow noopener" target="_blank">Kenneth Ooi</a>, <em>Karn N. Watcharasupat</em>, <a href="https://www.linkedin.com/in/lambhan" rel="external nofollow noopener" target="_blank">Bhan Lam</a>, Zhen-Ting Ong, and <a href="https://www3.ntu.edu.sg/home/ewsgan/" rel="external nofollow noopener" target="_blank">Woon-Seng Gan</a> </div> <div class="periodical"> <em>In Proceedings of the 2023 International Conference on Acoustics, Speech, and Signal Processing </em> , Jun 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="1" data-badge-type="2" data-badge-popover="right" data-doi="10.1109/ICASSP49357.2023.10094866"></span> <span class="__dimensions_badge_embed__" data-doi="10.1109/ICASSP49357.2023.10094866" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=5-5SQ9EAAAAJ&amp;citation_for_view=5-5SQ9EAAAAJ:ULOm3_A8WrAC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-7-4285F4?logo=googlescholar&amp;labelColor=beige" alt="7 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p> Autonomous soundscape augmentation systems typically use trained models to pick optimal maskers to effect a desired perceptual change. While acoustic information is paramount to such systems, contextual information, including participant demographics and the visual environment, also influences acoustic perception. Hence, we propose modular modifications to an existing attention-based deep neural network, to allow early, mid-level, and late feature fusion of participant-linked, visual, and acoustic features. Ablation studies on module configurations and corresponding fusion methods using the ARAUS dataset show that contextual features improve the model performance in a statistically significant manner on the normalized ISO Pleasantness, to a mean squared error of 0.1194\textpm0.0012 for the best-performing all-modality model, against 0.1217 \textpm 0.0009 for the audio-only model. Soundscape augmentation systems can thereby leverage multimodal inputs for improved performance. We also investigate the impact of individual participant-linked factors using trained models to illustrate improvements in model explainability. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Ooi2023AutonomousSoundscapeAugmentation</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{
  		Autonomous Soundscape Augmentation with Multimodal Fusion of Visual and
  		Participant-linked Inputs
  	}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ooi, Kenneth and Watcharasupat, Karn N. and Lam, Bhan and Ong, Zhen-Ting and Gan, Woon-Seng}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jun</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{
  		Proceedings of the 2023 International Conference on Acoustics, Speech, and
  		Signal Processing
  	}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ICASSP49357.2023.10094866}</span><span class="p">,</span>
  <span class="na">google_scholar_id</span> <span class="p">=</span> <span class="s">{ULOm3_A8WrAC}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#b509ac"> <div>Patent</div> </abbr> </div> <div id="Ooi2022SoundscapeAugmentationMethodPCT" class="col-sm-8"> <div class="title">Soundscape augmentation system and method of forming the same</div> <div class="author"> <a href="https://www.linkedin.com/in/kenneth-ooi-31919b39" rel="external nofollow noopener" target="_blank">Wen Rui Kenneth Ooi</a>, <em>Karn Watcharasupat</em>, <a href="https://www.linkedin.com/in/lambhan" rel="external nofollow noopener" target="_blank">Bhan Lam</a>, Zhen Ting Ong, Trevor Martens Zhi Ming Wong, and <a href="https://www3.ntu.edu.sg/home/ewsgan/" rel="external nofollow noopener" target="_blank">Woon Seng Gan</a> </div> <div class="periodical"> WO Patent App. PCT/SG2023/050289, Filed Apr 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://patents.google.com/patent/WO2023211385A1" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="1" data-badge-type="2" data-badge-popover="right" data-altmetric-id="true"></span> <span class="__dimensions_badge_embed__" data-id="true" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=5-5SQ9EAAAAJ&amp;citation_for_view=5-5SQ9EAAAAJ:QIV2ME_5wuYC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-0-4285F4?logo=googlescholar&amp;labelColor=beige" alt="0 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Various embodiments may provide a soundscape augmentation system. The soundscape augmentation system may include a data acquisition system configured to provide ambient soundscape data. The soundscape augmentation system may also include a database including a plurality of masker configurations. The soundscape augmentation system may further include a perceptual attribute predictor coupled to the data acquisition system and the database, the perceptual attribute predictor configured to generate predictions representing perception on one or more pre-defined perceptual attribute scales for each masker configuration of the plurality of masker configurations based on the ambient soundscape data. The soundscape augmentation system may additionally include a masker configuration ranking system configured to determine one or more optimal masker configurations based on the predictions generated by the perceptual attribute predictor. The soundscape augmentation system may also include a playback system configured to play back or reproduce the one or more optimal masker configurations. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@patent</span><span class="p">{</span><span class="nl">Ooi2022SoundscapeAugmentationMethodPCT</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ooi, Wen Rui Kenneth and Watcharasupat, Karn and Lam, Bhan and Ong, Zhen Ting and Wong, Trevor Martens Zhi Ming and Gan, Woon Seng}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Soundscape augmentation system and method of forming the same}</span><span class="p">,</span>
  <span class="na">nationality</span> <span class="p">=</span> <span class="s">{WO}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{PCT/SG2023/050289}</span><span class="p">,</span>
  <span class="na">dayfiled</span> <span class="p">=</span> <span class="s">{26}</span><span class="p">,</span>
  <span class="na">monthfiled</span> <span class="p">=</span> <span class="nv">apr</span><span class="p">,</span>
  <span class="na">yearfiled</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">day</span> <span class="p">=</span> <span class="s">{26}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">apr</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">google_scholar_id</span> <span class="p">=</span> <span class="s">{QIV2ME_5wuYC}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#00369f"> <div>Journal</div> </abbr> </div> <div id="Watcharasupat2022QuantitativeEvaluationApproach" class="col-sm-8"> <div class="title"> Quantitative Evaluation Approach for Translation of Perceptual Soundscape Attributes: Initial Application to the Thai Language </div> <div class="author"> <em>Karn N. Watcharasupat</em>, Sureenate Jaratjarungkiat, <a href="https://www.linkedin.com/in/lambhan" rel="external nofollow noopener" target="_blank">Bhan Lam</a>, Sujinat Jitwiriyanont, <a href="https://www.linkedin.com/in/kenneth-ooi-31919b39" rel="external nofollow noopener" target="_blank">Kenneth Ooi</a>, Zhen Ting Ong, Nitipong Pichetpan, Kanyanut Akaratham, Titima Suthiwan, Monthita Rojtinnakorn, and <a href="https://www3.ntu.edu.sg/home/ewsgan/" rel="external nofollow noopener" target="_blank">Woon Seng Gan</a> </div> <div class="periodical"> <em>Applied Acoustics</em>, Nov 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2203.12245" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.sciencedirect.com/science/article/abs/pii/S0003682X2200336X" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="1" data-badge-type="2" data-badge-popover="right" data-doi="10.1016/j.apacoust.2022.108962"></span> <span class="__dimensions_badge_embed__" data-doi="10.1016/j.apacoust.2022.108962" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=5-5SQ9EAAAAJ&amp;citation_for_view=5-5SQ9EAAAAJ:Se3iqnhoufwC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-26-4285F4?logo=googlescholar&amp;labelColor=beige" alt="26 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p> Translation of perceptual soundscape attributes from one language to another remains a challenging task that requires a high degree of fidelity in both psychoacoustic and psycholinguistic senses across the target population. Due to the inherently subjective nature of human perception, translating soundscape attributes using only small focus group discussions or expert panels could lead to translations with psycholinguistic meanings that, in a non-expert setting, deviate or distort from that of the source language. In this work, we present a quantitative evaluation method based on the circumplex model of soundscape perception to assess the overall translation quality. By establishing a set of criteria for evaluating the linguistic and psychometric properties of the translation candidates, statistical analyses can be performed to objectively assess specific strengths and weaknesses of the translation candidates before committing to listening tests or more involved validation experiments. As an initial application domain, we demonstrated the use of the quantitative evaluation framework in the context of an English-to-Thai translation of soundscape attributes. A total of 31 participants who are bilingual in English and Thai were recruited to assess the translation candidates. Subsequent statistical analysis of the evaluation scores revealed acoustico-psycholinguistic properties of the translation candidates which were not previously identified by the expert panel and facilitated a more objective selection of the final translations for subsequent usage. Additionally, with specific biases of the final translations determined numerically, mathematical and statistical techniques for corrections of the survey data may be employed in the future to improve cross-lingual compatibility in soundscape evaluation. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Watcharasupat2022QuantitativeEvaluationApproach</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{
  		Quantitative Evaluation Approach for Translation of Perceptual Soundscape
  		Attributes: Initial Application to the Thai Language
  	}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Watcharasupat, Karn N. and Jaratjarungkiat, Sureenate and Lam, Bhan and Jitwiriyanont, Sujinat and Ooi, Kenneth and Ong, Zhen Ting and Pichetpan, Nitipong and Akaratham, Kanyanut and Suthiwan, Titima and Rojtinnakorn, Monthita and Gan, Woon Seng}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Applied Acoustics}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Elsevier Ltd}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{200}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1016/j.apacoust.2022.108962}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{1872910X}</span><span class="p">,</span>
  <span class="na">google_scholar_id</span> <span class="p">=</span> <span class="s">{Se3iqnhoufwC}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#00369f"> <div>Journal</div> </abbr> </div> <div id="Lam2022CrossingLinguisticCauseway" class="col-sm-8"> <div class="title"> Crossing the Linguistic Causeway: A Binational Approach for Translating Soundscape Attributes to Bahasa Melayu </div> <div class="author"> <a href="https://www.linkedin.com/in/lambhan" rel="external nofollow noopener" target="_blank">Bhan Lam</a>, Julia Chieng, <em>Karn N. Watcharasupat</em>, <a href="https://www.linkedin.com/in/kenneth-ooi-31919b39" rel="external nofollow noopener" target="_blank">Kenneth Ooi</a>, Zhen-Ting Ong, Joo Young Hong, and <a href="https://www3.ntu.edu.sg/home/ewsgan/" rel="external nofollow noopener" target="_blank">Woon-Seng Gan</a> </div> <div class="periodical"> <em>Applied Acoustics</em>, Oct 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="1" data-badge-type="2" data-badge-popover="right" data-doi="10.1016/j.apacoust.2022.108976"></span> <span class="__dimensions_badge_embed__" data-doi="10.1016/j.apacoust.2022.108976" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=5-5SQ9EAAAAJ&amp;citation_for_view=5-5SQ9EAAAAJ:MXK_kJrjxJIC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-22-4285F4?logo=googlescholar&amp;labelColor=beige" alt="22 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p> Translation of perceptual descriptors such as the perceived affective quality attributes in the soundscape standard (ISO/TS 12913–2:2018) is an inherently intricate task, especially if the target language is used in multiple countries. Despite geographical proximity and a shared language of Bahasa Melayu (Standard Malay), differences in culture and language education policies between Singapore and Malaysia could invoke peculiarities in the affective appraisal of sounds. To generate provisional translations of the eight perceived affective attributes — eventful, vibrant, pleasant, calm, uneventful, monotonous, annoying, and chaotic — into Bahasa Melayu that is applicable in both Singapore and Malaysia, a binational expert-led approach supplemented by a quantitative evaluation framework was adopted. A set of preliminary translation candidates were developed via a four-stage process, firstly by a qualified translator, which was then vetted by linguistics experts, followed by examination via an experiential evaluation, and finally reviewed by the core research team. A total of 66 participants were then recruited cross-nationally to quantitatively evaluate the preliminary translation candidates. Of the eight attributes, cross-national differences were observed only in the translation of annoying. For instance, menjengkelkan was found to be significantly less understood in Singapore than in Malaysia, as well as less understandable than membingitkan within Singapore. Results of the quantitative evaluation also revealed the imperfect nature of foreign language translations for perceptual descriptors, which suggests a possibility for exploring corrective measures. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Lam2022CrossingLinguisticCauseway</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{
  		Crossing the Linguistic Causeway: A Binational Approach for Translating
  		Soundscape Attributes to Bahasa Melayu
  	}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lam, Bhan and Chieng, Julia and Watcharasupat, Karn N. and Ooi, Kenneth and Ong, Zhen-Ting and Hong, Joo Young and Gan, Woon-Seng}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Applied Acoustics}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{199}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{108976}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1016/j.apacoust.2022.108976}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{0003-682X}</span><span class="p">,</span>
  <span class="na">google_scholar_id</span> <span class="p">=</span> <span class="s">{MXK_kJrjxJIC}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#009f06"> <div>Conference</div> </abbr> </div> <div id="Ong2022UHearValidationUHear" class="col-sm-8"> <div class="title"> Do uHear? Validation of uHear App for Preliminary Screening of Hearing Ability in Soundscape Studies </div> <div class="author"> Zhen-Ting Ong, <a href="https://www.linkedin.com/in/lambhan" rel="external nofollow noopener" target="_blank">Bhan Lam</a>, <a href="https://www.linkedin.com/in/kenneth-ooi-31919b39" rel="external nofollow noopener" target="_blank">Kenneth Ooi</a>, <em>Karn N. Watcharasupat</em>, Trevor Wong, and <a href="https://www3.ntu.edu.sg/home/ewsgan/" rel="external nofollow noopener" target="_blank">Woon-Seng Gan</a> </div> <div class="periodical"> <em>In Proceedings of the 24th International Congress on Acoustics</em> , Oct 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="1" data-badge-type="2" data-badge-popover="right" data-doi="10.21979/n9/jqdi6f"></span> <span class="__dimensions_badge_embed__" data-doi="10.21979/n9/jqdi6f" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=5-5SQ9EAAAAJ&amp;citation_for_view=5-5SQ9EAAAAJ:KlAtU1dfN6UC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-3-4285F4?logo=googlescholar&amp;labelColor=beige" alt="3 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p> Studies involving soundscape perception often exclude participants with hearing loss to prevent impaired perception from affecting experimental results. Participants are typically screened with pure tone audiometry, the "gold standard" for identifying and quantifying hearing loss at specific frequencies, and excluded if a study-dependent threshold is not met. However, procuring professional audiometric equipment for soundscape studies may be cost-ineffective, and manually performing audiometric tests is labour-intensive. Moreover, testing requirements for soundscape studies may not require sensitivities and specificities as high as that in a medical diagnosis setting. Hence, in this study, we investigate the effectiveness of the uHear app, an iOS application, as an affordable and automatic alternative to a conventional audiometer in screening participants for hearing loss for the purpose of soundscape studies or listening tests in general. Based on audiometric comparisons with the audiometer of 163 participants, the uHear app was found to have high precision (98.04 %) when using the World Health Organization (WHO) grading scheme for assessing normal hearing. Precision is further improved (98.69 %) when all frequencies assessed with the uHear app is considered in the grading, which lends further support to this cost-effective, automated alternative to screen for normal hearing. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Ong2022UHearValidationUHear</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{
  		Do uHear? Validation of uHear App for Preliminary Screening of Hearing
  		Ability in Soundscape Studies
  	}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ong, Zhen-Ting and Lam, Bhan and Ooi, Kenneth and Watcharasupat, Karn N. and Wong, Trevor and Gan, Woon-Seng}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 24th International Congress on Acoustics}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.21979/n9/jqdi6f}</span><span class="p">,</span>
  <span class="na">google_scholar_id</span> <span class="p">=</span> <span class="s">{KlAtU1dfN6UC}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#009f06"> <div>Conference</div> </abbr> </div> <div id="Ooi2022BenchmarkComparisonPerceptual" class="col-sm-8"> <div class="title"> A Benchmark Comparison of Perceptual Models for Soundscapes on a Large-Scale Augmented Soundscape Dataset </div> <div class="author"> <a href="https://www.linkedin.com/in/kenneth-ooi-31919b39" rel="external nofollow noopener" target="_blank">Kenneth Ooi</a>, <em>Karn N. Watcharasupat</em>, <a href="https://www.linkedin.com/in/lambhan" rel="external nofollow noopener" target="_blank">Bhan Lam</a>, Zhen-Ting Ong, and <a href="https://www3.ntu.edu.sg/home/ewsgan/" rel="external nofollow noopener" target="_blank">Woon-Seng Gan</a> </div> <div class="periodical"> <em>In Proceedings of the 24th International Congress on Acoustics</em> , Oct 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="1" data-badge-type="2" data-badge-popover="right" data-doi="10.21979/n9/9otevx"></span> <span class="__dimensions_badge_embed__" data-doi="10.21979/n9/9otevx" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=5-5SQ9EAAAAJ&amp;citation_for_view=5-5SQ9EAAAAJ:YOwf2qJgpHMC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-1-4285F4?logo=googlescholar&amp;labelColor=beige" alt="1 Google Scholar citations"> </a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Ooi2022BenchmarkComparisonPerceptual</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{
  		A Benchmark Comparison of Perceptual Models for Soundscapes on a Large-Scale
  		Augmented Soundscape Dataset
  	}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ooi, Kenneth and Watcharasupat, Karn N. and Lam, Bhan and Ong, Zhen-Ting and Gan, Woon-Seng}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 24th International Congress on Acoustics}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.21979/n9/9otevx}</span><span class="p">,</span>
  <span class="na">google_scholar_id</span> <span class="p">=</span> <span class="s">{YOwf2qJgpHMC}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#009f06"> <div>Conference</div> </abbr> </div> <div id="Wong2022DeploymentIoTSystem" class="col-sm-8"> <div class="title">Deployment of an IoT System for Adaptive In-Situ Soundscape Augmentation</div> <div class="author"> Trevor Wong*, <em>Karn N. Watcharasupat*</em>, <a href="https://www.linkedin.com/in/lambhan" rel="external nofollow noopener" target="_blank">Bhan Lam</a>, <a href="https://www.linkedin.com/in/kenneth-ooi-31919b39" rel="external nofollow noopener" target="_blank">Kenneth Ooi</a>, Zhen-Ting Ong, Furi Andi Karnapi, and <a href="https://www3.ntu.edu.sg/home/ewsgan/" rel="external nofollow noopener" target="_blank">Woon-Seng Gan</a> </div> <div class="periodical"> <em>In Proceedings of the 51st International Congress and Expo on Noise Control Engineering </em> , Aug 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2204.13890" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="1" data-badge-type="2" data-badge-popover="right" data-doi="10.3397/IN_2022_0290"></span> <span class="__dimensions_badge_embed__" data-doi="10.3397/IN_2022_0290" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=5-5SQ9EAAAAJ&amp;citation_for_view=5-5SQ9EAAAAJ:0EnyYjriUFMC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-7-4285F4?logo=googlescholar&amp;labelColor=beige" alt="7 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p> Soundscape augmentation is an emerging approach for noise mitigation by introducing additional sounds known as "maskers" to increase acoustic comfort. Traditionally, the choice of maskers is often predicated on expert guidance or post-hoc analysis which can be time-consuming and sometimes arbitrary. Moreover, this often results in a static set of maskers that are inflexible to the dynamic nature of real-world acoustic environments. Overcoming the inflexibility of traditional soundscape augmentation is twofold. First, given a snapshot of a soundscape, the system must be able to select an optimal masker without human supervision. Second, the system must also be able to react to changes in the acoustic environment with near real-time latency. In this work, we harness the combined prowess of cloud computing and the Internet of Things (IoT) to allow in-situ listening and playback using microcontrollers while delegating computationally expensive inference tasks to the cloud. In particular, a serverless cloud architecture was used for inference, ensuring near real-time latency and scalability without the need to provision computing resources. A working prototype of the system is currently being deployed in a public area experiencing high traffic noise, as well as undergoing public evaluation for future improvements. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Wong2022DeploymentIoTSystem</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Deployment of an IoT System for Adaptive In-Situ Soundscape Augmentation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wong*, Trevor and Watcharasupat*, Karn N. and Lam, Bhan and Ooi, Kenneth and Ong, Zhen-Ting and Karnapi, Furi Andi and Gan, Woon-Seng}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">aug</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{
  		Proceedings of the 51st International Congress and Expo on Noise Control
  		Engineering
  	}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.3397/IN_2022_0290}</span><span class="p">,</span>
  <span class="na">google_scholar_id</span> <span class="p">=</span> <span class="s">{0EnyYjriUFMC}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#00369f"> <div>Journal</div> </abbr> </div> <div id="Watcharasupat2022AutonomousInSituSoundscape" class="col-sm-8"> <div class="title"> Autonomous In-Situ Soundscape Augmentation via Joint Selection of Masker and Gain </div> <div class="author"> <em>Karn N. Watcharasupat</em>, <a href="https://www.linkedin.com/in/kenneth-ooi-31919b39" rel="external nofollow noopener" target="_blank">Kenneth Ooi</a>, <a href="https://www.linkedin.com/in/lambhan" rel="external nofollow noopener" target="_blank">Bhan Lam</a>, Trevor Wong, Zhen Ting Ong, and <a href="https://www3.ntu.edu.sg/home/ewsgan/" rel="external nofollow noopener" target="_blank">Woon Seng Gan</a> </div> <div class="periodical"> <em>IEEE Signal Processing Letters</em>, Jul 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2204.13883" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/9841611" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://api.zotero.org/users/7862008/publications/items/HGBUI9DY/file/view" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="1" data-badge-type="2" data-badge-popover="right" data-doi="10.1109/lsp.2022.3194419"></span> <span class="__dimensions_badge_embed__" data-doi="10.1109/lsp.2022.3194419" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=5-5SQ9EAAAAJ&amp;citation_for_view=5-5SQ9EAAAAJ:5nxA0vEk-isC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-11-4285F4?logo=googlescholar&amp;labelColor=beige" alt="11 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p> The selection of maskers and playback gain levels in an in-situ soundscape augmentation system is crucial to its effectiveness in improving the overall acoustic comfort of a given environment. Traditionally, the selection of appropriate maskers and gain levels has been informed by expert opinion, which may not be representative of the target population, or by listening tests, which can be time- and labor-intensive. Furthermore, the resulting static choices of masker and gain are often inflexible to dynamic real-world soundscapes. In this work, we utilized a deep learning model to perform joint selection of the optimal masker and its gain level for a given soundscape. The proposed model was designed with highly modular building blocks, allowing for an optimized inference process that can quickly search through a large number of masker-gain combinations. In addition, we introduced the use of feature-domain soundscape augmentation conditioned on the digital gain level, eliminating the computationally expensive waveform-domain mixing process during inference, as well as the tedious gain adjustment process required for new maskers. The proposed system was evaluated on a large-scale dataset of subjective responses to augmented soundscapes with 442 participants, with the best model achieving a mean squared error of <inline-formula><tex-math notation="LaTeX">}{0.122}}mathbf {}pm }{0.005}{</tex-math></inline-formula> on pleasantness score, validating the ability of the model to predict combined effect of the masker and its gain level on the perceptual pleasantness level. The proposed system thus allows in-situ or mixed-reality soundscape augmentation to be performed autonomously with near real-time latency while continuously accounting for changes in acoustic environments. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Watcharasupat2022AutonomousInSituSoundscape</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{
  		Autonomous In-Situ Soundscape Augmentation via Joint Selection of Masker and
  		Gain
  	}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Watcharasupat, Karn N. and Ooi, Kenneth and Lam, Bhan and Wong, Trevor and Ong, Zhen Ting and Gan, Woon Seng}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Signal Processing Letters}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{29}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1749--1753}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/lsp.2022.3194419}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{15582361}</span><span class="p">,</span>
  <span class="na">google_scholar_id</span> <span class="p">=</span> <span class="s">{5nxA0vEk-isC}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#009f06"> <div>Conference</div> </abbr> </div> <div id="Lam2022AssessmentCosteffectiveHeadphone" class="col-sm-8"> <div class="title"> Assessment of a Cost-Effective Headphone Calibration Procedure for Soundscape Evaluations </div> <div class="author"> <a href="https://www.linkedin.com/in/lambhan" rel="external nofollow noopener" target="_blank">Bhan Lam</a>, <a href="https://www.linkedin.com/in/kenneth-ooi-31919b39" rel="external nofollow noopener" target="_blank">Kenneth Ooi</a>, Zhen-Ting Ong, <em>Karn N. Watcharasupat</em>, Trevor Wong, and <a href="https://www3.ntu.edu.sg/home/ewsgan/" rel="external nofollow noopener" target="_blank">Woon-Seng Gan</a> </div> <div class="periodical"> <em>In Proceedings of the 24th International Congress on Acoustics</em> , Jul 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2207.12899" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="1" data-badge-type="2" data-badge-popover="right" data-altmetric-id="true"></span> <span class="__dimensions_badge_embed__" data-id="true" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=5-5SQ9EAAAAJ&amp;citation_for_view=5-5SQ9EAAAAJ:Zph67rFs4hoC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-7-4285F4?logo=googlescholar&amp;labelColor=beige" alt="7 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p> To increase the availability and adoption of the soundscape standard, a low-cost calibration procedure for reproduction of audio stimuli over headphones was proposed as part of the global “Soundscape Attributes Translation Project” (SATP) for validating ISO/TS~12913-2:2018 perceived affective quality (PAQ) attribute translations. A previous preliminary study revealed significant deviations from the intended equivalent continuous A-weighted sound pressure levels (Ł_{}text{A,eq}}\) using the open-circuit voltage (OCV) calibration procedure. For a more holistic human-centric perspective, the OCV method is further investigated here in terms of psychoacoustic parameters, including relevant exceedance levels to account for temporal effects on the same 27 stimuli from the SATP. Moreover, a within-subjects experiment with 36 participants was conducted to examine the effects of OCV calibration on the PAQ attributes in ISO/TS~12913-2:2018. Bland-Altman analysis of the objective indicators revealed large biases in the OCV method across all weighted sound level and loudness indicators; and roughness indicators at }SI{5}{}%} and }SI{10}{}%} exceedance levels. Significant perceptual differences due to the OCV method were observed in about }SI{20}{}%} of the stimuli, which did not correspond clearly with the biased acoustic indicators. A cautioned interpretation of the objective and perceptual differences due to small and unpaired samples nevertheless provide grounds for further investigation. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Lam2022AssessmentCosteffectiveHeadphone</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{
  		Assessment of a Cost-Effective Headphone Calibration Procedure for Soundscape
  		Evaluations
  	}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lam, Bhan and Ooi, Kenneth and Ong, Zhen-Ting and Watcharasupat, Karn N. and Wong, Trevor and Gan, Woon-Seng}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 24th International Congress on Acoustics}</span><span class="p">,</span>
  <span class="na">google_scholar_id</span> <span class="p">=</span> <span class="s">{Zph67rFs4hoC}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#00369f"> <div>Journal</div> </abbr> </div> <div id="Ooi2022SingaporeSoundscapeSite" class="col-sm-8"> <div class="title"> Singapore Soundscape Site Selection Survey (S5): Identification of Characteristic Soundscapes of Singapore via Weighted k-Means Clustering </div> <div class="author"> <a href="https://www.linkedin.com/in/kenneth-ooi-31919b39" rel="external nofollow noopener" target="_blank">Kenneth Ooi</a>, <a href="https://www.linkedin.com/in/lambhan" rel="external nofollow noopener" target="_blank">Bhan Lam</a>, Joo Young Hong, <em>Karn N. Watcharasupat</em>, Zhen Ting Ong, and <a href="https://www3.ntu.edu.sg/home/ewsgan/" rel="external nofollow noopener" target="_blank">Woon Seng Gan</a> </div> <div class="periodical"> <em>Sustainability</em>, Jun 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="1" data-badge-type="2" data-badge-popover="right" data-doi="10.3390/su14127485"></span> <span class="__dimensions_badge_embed__" data-doi="10.3390/su14127485" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=5-5SQ9EAAAAJ&amp;citation_for_view=5-5SQ9EAAAAJ:3fE2CSJIrl8C" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-6-4285F4?logo=googlescholar&amp;labelColor=beige" alt="6 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p> The ecological validity of soundscape studies usually rests on the choice of soundscapes that are representative of the perceptual space under investigation. For example, a soundscape pleasantness study might investigate locations with soundscapes ranging from “pleasant” to “annoying”. The choice of soundscapes is typically researcher led, but a participant-led process can reduce selection bias and improve result reliability. Hence, we propose a robust participant-led method to pinpoint characteristic soundscapes possessing arbitrary perceptual attributes. We validate our method by identifying Singaporean soundscapes spanning the perceptual quadrants generated from the “Pleasantness” and “Eventfulness” axes of the ISO 12913-2 circumplex model of soundscape perception, as perceived by local experts. From memory and experience, 67 participants first selected locations corresponding to each perceptual quadrant in each major planning region of Singapore. We then performed weighted k-means clustering on the selected locations, with weights for each location derived from previous frequencies and durations spent in each location by each participant. Weights hence acted as proxies for participant confidence. In total, 62 locations were thereby identified as suitable locations with characteristic soundscapes for further research utilizing the ISO 12913-2 perceptual quadrants. Audio–visual recordings and acoustic characterization of the soundscapes will be made in a future study. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Ooi2022SingaporeSoundscapeSite</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{
  		Singapore Soundscape Site Selection Survey (S5): Identification of
  		Characteristic Soundscapes of Singapore via Weighted k-Means Clustering
  	}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ooi, Kenneth and Lam, Bhan and Hong, Joo Young and Watcharasupat, Karn N. and Ong, Zhen Ting and Gan, Woon Seng}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jun</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Sustainability}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{MDPI}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{14}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{12}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.3390/su14127485}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{20711050}</span><span class="p">,</span>
  <span class="na">google_scholar_id</span> <span class="p">=</span> <span class="s">{3fE2CSJIrl8C}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#009f06"> <div>Conference</div> </abbr> </div> <div id="Watcharasupat2022EndtoEndComplexValuedMultidilated" class="col-sm-8"> <div class="title"> End-to-End Complex-Valued Multidilated Convolutional Neural Network for Joint Acoustic Echo Cancellation and Noise Suppression </div> <div class="author"> <em>Karn N. Watcharasupat</em>, <a href="https://sg.linkedin.com/in/nguyen-thi-ngoc-tho-15102012" rel="external nofollow noopener" target="_blank">Thi Ngoc Tho Nguyen</a>, <a href="https://www3.ntu.edu.sg/home/ewsgan/" rel="external nofollow noopener" target="_blank">Woon-Seng Gan</a>, Shengkui Zhao, and Bin Ma </div> <div class="periodical"> <em>In Proceedings of the 2022 IEEE International Conference on Acoustics, Speech and Signal Processing </em> , May 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2110.00745" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/9747034/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://ieeexplore.ieee.org/document/9747034/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="1" data-badge-type="2" data-badge-popover="right" data-doi="10.1109/icassp43922.2022.9747034"></span> <span class="__dimensions_badge_embed__" data-doi="10.1109/icassp43922.2022.9747034" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=5-5SQ9EAAAAJ&amp;citation_for_view=5-5SQ9EAAAAJ:YsMSGLbcyi4C" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-14-4285F4?logo=googlescholar&amp;labelColor=beige" alt="14 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p> Echo and noise suppression is an integral part of a full-duplex communication system. Many recent acoustic echo cancellation (AEC) systems rely on a separate adaptive filtering module for linear echo suppression and a neural module for residual echo suppression. However, not only do adaptive filtering modules require convergence and remain susceptible to changes in acoustic environments, but this two-stage framework also often introduces unnecessary delays to the AEC system when neural modules are already capable of both linear and nonlinear echo suppression. In this paper, we exploit the offset-compensating ability of complex time-frequency masks and propose an end-to-end complex-valued neural network architecture. The building block of the proposed model is a pseudocomplex extension based on the densely-connected multidilated DenseNet (D3Net) building block, resulting in a very small network of only 354K parameters. The architecture utilized the multi-resolution nature of the D3Net building blocks to eliminate the need for pooling, allowing the network to extract features using large receptive fields without any loss of output resolution. We also propose a dual-mask technique for joint echo and noise suppression with simultaneous speech enhancement. Evaluation on both synthetic and real test sets demonstrated promising results across multiple energy-based metrics and perceptual proxies. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Watcharasupat2022EndtoEndComplexValuedMultidilated</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{
  		End-to-End Complex-Valued Multidilated Convolutional Neural Network for Joint
  		Acoustic Echo Cancellation and Noise Suppression
  	}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Watcharasupat, Karn N. and Nguyen, Thi Ngoc Tho and Gan, Woon-Seng and Zhao, Shengkui and Ma, Bin}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{
  		Proceedings of the 2022 IEEE International Conference on Acoustics, Speech
  		and Signal Processing
  	}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{656--660}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/icassp43922.2022.9747034}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{978-1-66540-540-9}</span><span class="p">,</span>
  <span class="na">google_scholar_id</span> <span class="p">=</span> <span class="s">{YsMSGLbcyi4C}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#00369f"> <div>Journal</div> </abbr> </div> <div id="Nguyen2022SALSASpatialCueAugmented" class="col-sm-8"> <div class="title"> SALSA: Spatial Cue-Augmented Log-Spectrogram Features for Polyphonic Sound Event Localization and Detection </div> <div class="author"> <a href="https://sg.linkedin.com/in/nguyen-thi-ngoc-tho-15102012" rel="external nofollow noopener" target="_blank">Thi Ngoc Tho Nguyen</a>, <em>Karn N. Watcharasupat</em> , Ngoc Khanh Nguyen, <a href="https://ece.illinois.edu/about/directory/faculty/dl-jones" rel="external nofollow noopener" target="_blank">Douglas L. Jones</a>, and <a href="https://www3.ntu.edu.sg/home/ewsgan/" rel="external nofollow noopener" target="_blank">Woon-Seng Gan</a> </div> <div class="periodical"> <em>IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, May 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="1" data-badge-type="2" data-badge-popover="right" data-doi="10.1109/taslp.2022.3173054"></span> <span class="__dimensions_badge_embed__" data-doi="10.1109/taslp.2022.3173054" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=5-5SQ9EAAAAJ&amp;citation_for_view=5-5SQ9EAAAAJ:W7OEmFMy1HYC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-55-4285F4?logo=googlescholar&amp;labelColor=beige" alt="55 Google Scholar citations"> </a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Nguyen2022SALSASpatialCueAugmented</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{
  		SALSA: Spatial Cue-Augmented Log-Spectrogram Features for Polyphonic Sound
  		Event Localization and Detection
  	}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Nguyen, Thi Ngoc Tho and Watcharasupat, Karn N. and Nguyen, Ngoc Khanh and Jones, Douglas L. and Gan, Woon-Seng}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE/ACM Transactions on Audio, Speech, and Language Processing}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{30}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1749--1762}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/taslp.2022.3173054}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2329-9304}</span><span class="p">,</span>
  <span class="na">google_scholar_id</span> <span class="p">=</span> <span class="s">{W7OEmFMy1HYC}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#009f06"> <div>Conference</div> </abbr> </div> <div id="Lam2022PreliminaryAssessmentCosteffective" class="col-sm-8"> <div class="title"> Preliminary Assessment of a Cost-Effective Headphone Calibration Procedure for Soundscape Evaluations </div> <div class="author"> <a href="https://www.linkedin.com/in/lambhan" rel="external nofollow noopener" target="_blank">Bhan Lam</a>, <a href="https://www.linkedin.com/in/kenneth-ooi-31919b39" rel="external nofollow noopener" target="_blank">Kenneth Ooi</a>, <em>Karn N. Watcharasupat</em>, Zhen-Ting Ong, Yun-Ting Lau, Trevor Wong, and <a href="https://www3.ntu.edu.sg/home/ewsgan/" rel="external nofollow noopener" target="_blank">Woon-Seng Gan</a> </div> <div class="periodical"> <em>In Proceedings of the 28th International Congress on Sound and Vibration</em> , May 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2205.04728" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="1" data-badge-type="2" data-badge-popover="right" data-altmetric-id="true"></span> <span class="__dimensions_badge_embed__" data-id="true" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=5-5SQ9EAAAAJ&amp;citation_for_view=5-5SQ9EAAAAJ:8k81kl-MbHgC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-5-4285F4?logo=googlescholar&amp;labelColor=beige" alt="5 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p> The introduction of ISO 12913-2:2018 has provided a framework for standardized data collection and reporting procedures for soundscape practitioners. A strong emphasis was placed on the use of calibrated head and torso simulators (HATS) for binaural audio capture to obtain an accurate subjective impression and acoustic measure of the soundscape under evaluation. To auralise the binaural recordings as recorded or at set levels, the audio stimuli and the headphone setup are usually calibrated with a HATS. However, calibrated HATS are too financially prohibitive for most research teams, inevitably diminishing the availability of the soundscape standard. With the increasing availability of soundscape binaural recording datasets, and the importance of cross-cultural validation of the soundscape ISO standards, e.g.} via the Soundscape Attributes Translation Project (SATP), it is imperative to assess the suitability of cost-effective headphone calibration methods to maximise availability without severely compromising on accuracy. Hence, this study objectively examines an open-circuit voltage (OCV) calibration method in comparison to a calibrated HATS on various soundcard and headphone combinations. Preliminary experiments found that calibration with the OCV method differed significantly from the reference binaural recordings in sound pressure levels, whereas negligible differences in levels were observed with the HATS calibration. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Lam2022PreliminaryAssessmentCosteffective</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{
  		Preliminary Assessment of a Cost-Effective Headphone Calibration Procedure
  		for Soundscape Evaluations
  	}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lam, Bhan and Ooi, Kenneth and Watcharasupat, Karn N. and Ong, Zhen-Ting and Lau, Yun-Ting and Wong, Trevor and Gan, Woon-Seng}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 28th International Congress on Sound and Vibration}</span><span class="p">,</span>
  <span class="na">google_scholar_id</span> <span class="p">=</span> <span class="s">{8k81kl-MbHgC}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#009f06"> <div>Conference</div> </abbr> </div> <div id="Nguyen2022SALSALiteFastEffective" class="col-sm-8"> <div class="title"> SALSA-Lite: A Fast and Effective Feature for Polyphonic Sound Event Localization and Detection with Microphone Arrays </div> <div class="author"> <a href="https://sg.linkedin.com/in/nguyen-thi-ngoc-tho-15102012" rel="external nofollow noopener" target="_blank">Thi Ngoc Tho Nguyen</a>, <a href="https://ece.illinois.edu/about/directory/faculty/dl-jones" rel="external nofollow noopener" target="_blank">Douglas L. Jones</a>, <em>Karn N. Watcharasupat</em>, Huy Phan, and <a href="https://www3.ntu.edu.sg/home/ewsgan/" rel="external nofollow noopener" target="_blank">Woon-Seng Gan</a> </div> <div class="periodical"> <em>In Proceedings of the 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) </em> , May 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="1" data-badge-type="2" data-badge-popover="right" data-doi="10.1109/icassp43922.2022.9746132"></span> <span class="__dimensions_badge_embed__" data-doi="10.1109/icassp43922.2022.9746132" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=5-5SQ9EAAAAJ&amp;citation_for_view=5-5SQ9EAAAAJ:_FxGoFyzp5QC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-47-4285F4?logo=googlescholar&amp;labelColor=beige" alt="47 Google Scholar citations"> </a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Nguyen2022SALSALiteFastEffective</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{
  		SALSA-Lite: A Fast and Effective Feature for Polyphonic Sound Event
  		Localization and Detection with Microphone Arrays
  	}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Nguyen, Thi Ngoc Tho and Jones, Douglas L. and Watcharasupat, Karn N. and Phan, Huy and Gan, Woon-Seng}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{
  		Proceedings of the 2022 IEEE International Conference on Acoustics, Speech
  		and Signal Processing (ICASSP)
  	}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{716--720}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/icassp43922.2022.9746132}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{2379-190X}</span><span class="p">,</span>
  <span class="na">google_scholar_id</span> <span class="p">=</span> <span class="s">{_FxGoFyzp5QC}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#009f06"> <div>Conference</div> </abbr> </div> <div id="Ooi2022ProbablyPleasantNeuralProbabilistic" class="col-sm-8"> <div class="title"> Probably Pleasant? A Neural-Probabilistic Approach to Automatic Masker Selection for Urban Soundscape Augmentation </div> <div class="author"> <a href="https://www.linkedin.com/in/kenneth-ooi-31919b39" rel="external nofollow noopener" target="_blank">Kenneth Ooi</a>, <em>Karn N. Watcharasupat</em>, <a href="https://www.linkedin.com/in/lambhan" rel="external nofollow noopener" target="_blank">Bhan Lam</a>, Zhen-Ting Ong, and <a href="https://www3.ntu.edu.sg/home/ewsgan/" rel="external nofollow noopener" target="_blank">Woon-Seng Gan</a> </div> <div class="periodical"> <em>In Proceedings of the 2022 IEEE International Conference on Acoustics, Speech and Signal Processing </em> , May 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="1" data-badge-type="2" data-badge-popover="right" data-doi="10.1109/icassp43922.2022.9746897"></span> <span class="__dimensions_badge_embed__" data-doi="10.1109/icassp43922.2022.9746897" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=5-5SQ9EAAAAJ&amp;citation_for_view=5-5SQ9EAAAAJ:hqOjcs7Dif8C" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-12-4285F4?logo=googlescholar&amp;labelColor=beige" alt="12 Google Scholar citations"> </a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Ooi2022ProbablyPleasantNeuralProbabilistic</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{
  		Probably Pleasant? A Neural-Probabilistic Approach to Automatic Masker
  		Selection for Urban Soundscape Augmentation
  	}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ooi, Kenneth and Watcharasupat, Karn N. and Lam, Bhan and Ong, Zhen-Ting and Gan, Woon-Seng}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{
  		Proceedings of the 2022 IEEE International Conference on Acoustics, Speech
  		and Signal Processing
  	}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/icassp43922.2022.9746897}</span><span class="p">,</span>
  <span class="na">google_scholar_id</span> <span class="p">=</span> <span class="s">{hqOjcs7Dif8C}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#009f06"> <div>Conference</div> </abbr> </div> <div id="Zhao2022FRCRNBoostingFeature" class="col-sm-8"> <div class="title"> FRCRN: Boosting Feature Representation Using Frequency Recurrence for Monaural Speech Enhancement </div> <div class="author"> Shengkui Zhao, Bin Ma, <em>Karn N. Watcharasupat</em>, and <a href="https://www3.ntu.edu.sg/home/ewsgan/" rel="external nofollow noopener" target="_blank">Woon-Seng Gan</a> </div> <div class="periodical"> <em>In Proceedings of the 2022 IEEE International Conference on Acoustics, Speech and Signal Processing </em> , May 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="1" data-badge-type="2" data-badge-popover="right" data-doi="10.1109/icassp43922.2022.9747578"></span> <span class="__dimensions_badge_embed__" data-doi="10.1109/icassp43922.2022.9747578" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=5-5SQ9EAAAAJ&amp;citation_for_view=5-5SQ9EAAAAJ:UebtZRa9Y70C" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-77-4285F4?logo=googlescholar&amp;labelColor=beige" alt="77 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p> Convolutional recurrent networks (CRN) integrating a convolutional encoder-decoder (CED) structure and a recurrent structure have achieved promising performance for monaural speech enhancement. However, feature representation across frequency context is highly constrained due to limited receptive fields in the convolutions of CED. In this paper, we propose a convolutional recurrent encoder-decoder (CRED) structure to boost feature representation along the frequency axis. The CRED applies frequency recurrence on 3D convolutional feature maps along the frequency axis following each convolution, therefore, it is capable of catching long-range frequency correlations and enhancing feature representations of speech inputs. The proposed frequency recurrence is realized efficiently using a feedforward sequential memory network (FSMN). Besides the CRED, we insert two stacked FSMN layers between the encoder and the decoder to model further temporal dynamics. We name the proposed framework as Frequency Recurrent CRN (FRCRN). We design FRCRN to predict complex Ideal Ratio Mask (cIRM) in complex-valued domain and optimize FRCRN using both time-frequency-domain and time-domain losses. Our proposed approach achieved state-of-the-art performance on wideband benchmark datasets and achieved 2nd place for the real-time fullband track in terms of Mean Opinion Score (MOS) and Word Accuracy (WAcc) in the ICASSP 2022 Deep Noise Suppression (DNS) challenge. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Zhao2022FRCRNBoostingFeature</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{
  		FRCRN: Boosting Feature Representation Using Frequency Recurrence for
  		Monaural Speech Enhancement
  	}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhao, Shengkui and Ma, Bin and Watcharasupat, Karn N. and Gan, Woon-Seng}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{
  		Proceedings of the 2022 IEEE International Conference on Acoustics, Speech
  		and Signal Processing
  	}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{9281--9285}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/icassp43922.2022.9747578}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{978-1-66540-540-9}</span><span class="p">,</span>
  <span class="na">google_scholar_id</span> <span class="p">=</span> <span class="s">{UebtZRa9Y70C}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#00369f"> <div>Journal</div> </abbr> </div> <div id="Watcharasupat2022LatteCrossframeworkPython" class="col-sm-8"> <div class="title"> Latte: Cross-framework Python Package for Evaluation of Latent-Based Generative Models </div> <div class="author"> <em>Karn N. Watcharasupat</em>, Junyoung Lee, and <a href="https://www.alexanderlerch.com/" rel="external nofollow noopener" target="_blank">Alexander Lerch</a> </div> <div class="periodical"> <em>Software Impacts</em>, Feb 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.sciencedirect.com/science/article/pii/S2665963822000033" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://api.zotero.org/users/7862008/publications/items/DNJQSAW3/file/view" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://musicinformatics.gatech.edu/project/latte-cross-framework-python-package-for-evaluation-of-latent-based-generative-models/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Blog</a> <a href="https://github.com/karnwatcharasupat/latte" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="1" data-badge-type="2" data-badge-popover="right" data-doi="10.1016/j.simpa.2022.100222"></span> <span class="__dimensions_badge_embed__" data-doi="10.1016/j.simpa.2022.100222" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=5-5SQ9EAAAAJ&amp;citation_for_view=5-5SQ9EAAAAJ:roLk4NBRz8UC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-3-4285F4?logo=googlescholar&amp;labelColor=beige" alt="3 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p> Latte (for LATent Tensor Evaluation) is a Python library for evaluation of latent-based generative models in the fields of disentanglement learning and controllable generation. Latte is compatible with both PyTorch and TensorFlow/Keras, and provides both functional and modular APIs that can be easily extended to support other deep learning frameworks. Using NumPy-based and framework-agnostic implementation, Latte ensures reproducible, consistent, and deterministic metric calculations regardless of the deep learning framework of choice. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Watcharasupat2022LatteCrossframeworkPython</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{
  		Latte: Cross-framework Python Package for Evaluation of Latent-Based
  		Generative Models
  	}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Watcharasupat, Karn N. and Lee, Junyoung and Lerch, Alexander}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">feb</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Software Impacts}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{11}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{100222}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1016/j.simpa.2022.100222}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2665-9638}</span><span class="p">,</span>
  <span class="na">google_scholar_id</span> <span class="p">=</span> <span class="s">{roLk4NBRz8UC}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#0076df"> <div>Thesis</div> </abbr> </div> <div id="Watcharasupat2021ControllableMusicSupervised" class="col-sm-8"> <div class="title"> Controllable Music: Supervised Learning of Disentangled Representations for Music Generation </div> <div class="author"> <em>Karn N. Watcharasupat</em> </div> <div class="periodical"> Dec 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://dr.ntu.edu.sg/handle/10356/153200" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="1" data-badge-type="2" data-badge-popover="right" data-altmetric-id="true"></span> <span class="__dimensions_badge_embed__" data-id="true" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=5-5SQ9EAAAAJ&amp;citation_for_view=5-5SQ9EAAAAJ:LkGwnXOMwfcC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-2-4285F4?logo=googlescholar&amp;labelColor=beige" alt="2 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p> Controllability, despite being a much-desired property of a generative model, remains an ill-defined concept that is difficult to measure. In the context of neural music generation, a controllable system often implies an intuitive interaction between human agents and the neural model, allowing the relatively opaque neural model to be controlled by a human in a semantically understandable manner. In this work, we aim to tackle controllable music generation in the raw audio domain, which is significantly less attempted compared to the symbolic domain. Specifically, we focus on controlling multiple continuous, potentially interdependent timbral attributes of a musical note using a variational autoencoder (VAE) framework, and the necessary groundwork research needed to support the goal. Specifically, this work consists of three main parts. The first formulates the concept of <i>controllability</i> and how to evaluate a latent manifold of deep generative models in the presence of multiple interdependent attributes. The second focuses on the development of a composite latent space architecture for VAE, in order to allow encoding of interdependent attributes which having an easily sampled disentangled prior. Proofs of concept work for the second part was performed on several standard vision disentanglement learning datasets. Finally, the last part applies the composite latent space model on music generation in the raw audio domain and discusses the evaluation of the model against the criteria defined in the first part of this project. All in all, given the relatively uncharted nature of the controllable generation in the raw audio domain, this project provides a foundational work for the evaluation of controllable generation as a whole, and a promising proof of concept for musical audio generation with timbral control using variational autoencoders. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@thesis</span><span class="p">{</span><span class="nl">Watcharasupat2021ControllableMusicSupervised</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{
  		Controllable Music: Supervised Learning of Disentangled Representations for
  		Music Generation
  	}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Watcharasupat, Karn N.}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Singapore}</span><span class="p">,</span>
  <span class="na">type</span> <span class="p">=</span> <span class="s">{Final {{Year Project}} ({{FYP}})}</span><span class="p">,</span>
  <span class="na">lccn</span> <span class="p">=</span> <span class="s">{CY3001-211}</span><span class="p">,</span>
  <span class="na">school</span> <span class="p">=</span> <span class="s">{Nanyang Technological University}</span><span class="p">,</span>
  <span class="na">google_scholar_id</span> <span class="p">=</span> <span class="s">{LkGwnXOMwfcC}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Ooi2021StronglyLabelledPolyphonicDataset" class="col-sm-8"> <div class="title"> A Strongly-Labelled Polyphonic Dataset of Urban Sounds with Spatiotemporal Context </div> <div class="author"> <a href="https://www.linkedin.com/in/kenneth-ooi-31919b39" rel="external nofollow noopener" target="_blank">Kenneth Ooi</a>, <em>Karn N. Watcharasupat</em>, Santi Peksi, Furi Andi Karnapi, Zhen-Ting Ong, Danny Chua, Hui-Wen Leow, Li-Long Kwok, Xin-Lei Ng, Zhen-Ann Loh, and <a href="https://www3.ntu.edu.sg/home/ewsgan/" rel="external nofollow noopener" target="_blank">Woon-Seng Gan</a> </div> <div class="periodical"> <em>In Proceedings of the 13th Asia Pacific Signal and Information Processing Association Annual Summit and Conference </em> , Dec 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="1" data-badge-type="2" data-badge-popover="right" data-altmetric-id="true"></span> <span class="__dimensions_badge_embed__" data-id="true" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=5-5SQ9EAAAAJ&amp;citation_for_view=5-5SQ9EAAAAJ:WF5omc3nYNoC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-9-4285F4?logo=googlescholar&amp;labelColor=beige" alt="9 Google Scholar citations"> </a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Ooi2021StronglyLabelledPolyphonicDataset</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{
  		A Strongly-Labelled Polyphonic Dataset of Urban Sounds with Spatiotemporal
  		Context
  	}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ooi, Kenneth and Watcharasupat, Karn N. and Peksi, Santi and Karnapi, Furi Andi and Ong, Zhen-Ting and Chua, Danny and Leow, Hui-Wen and Kwok, Li-Long and Ng, Xin-Lei and Loh, Zhen-Ann and Gan, Woon-Seng}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{
  		Proceedings of the 13th Asia Pacific Signal and Information Processing
  		Association Annual Summit and Conference
  	}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Asia Pacific Signal and Information Processing Association}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Tokyo, Japan}</span><span class="p">,</span>
  <span class="na">google_scholar_id</span> <span class="p">=</span> <span class="s">{WF5omc3nYNoC}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#ff3636"> <div>Ext. Abstract</div> </abbr> </div> <div id="Watcharasupat2021EvaluationLatentSpace" class="col-sm-8"> <div class="title"> Evaluation of Latent Space Disentanglement in the Presence of Interdependent Attributes </div> <div class="author"> <em>Karn N. Watcharasupat</em>, and <a href="https://www.alexanderlerch.com/" rel="external nofollow noopener" target="_blank">Alexander Lerch</a> </div> <div class="periodical"> <em>In Extended Abstracts of the Late-Breaking Demo Session of the 22nd International Society for Music Information Retrieval Conference </em> , Nov 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://archives.ismir.net/ismir2021/latebreaking/000002.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="1" data-badge-type="2" data-badge-popover="right" data-altmetric-id="true"></span> <span class="__dimensions_badge_embed__" data-id="true" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=5-5SQ9EAAAAJ&amp;citation_for_view=5-5SQ9EAAAAJ:eQOLeE2rZwMC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-5-4285F4?logo=googlescholar&amp;labelColor=beige" alt="5 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p> Controllable music generation with deep generative models has become increasingly reliant on disentanglement learning techniques. However, current disentanglement metrics, such as mutual information gap (MIG), are often inadequate and misleading when used for evaluating latent representations in the presence of interdependent semantic attributes often encountered in real-world music datasets. In this work, we propose a dependency-aware information metric as a drop-in replacement for MIG that accounts for the inherent relationship between semantic attributes. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Watcharasupat2021EvaluationLatentSpace</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{
  		Evaluation of Latent Space Disentanglement in the Presence of Interdependent
  		Attributes
  	}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Watcharasupat, Karn N. and Lerch, Alexander}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{
  		Extended Abstracts of the Late-Breaking Demo Session of the 22nd
  		International Society for Music Information Retrieval Conference
  	}</span><span class="p">,</span>
  <span class="na">google_scholar_id</span> <span class="p">=</span> <span class="s">{eQOLeE2rZwMC}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#ff3636"> <div>Ext. Abstract</div> </abbr> </div> <div id="Hung2021AVASpeechSMADStronglyLabelled" class="col-sm-8"> <div class="title"> AVASpeech-SMAD: A Strongly Labelled Speech and Music Activity Detection Dataset with Label Co-Occurrence </div> <div class="author"> <a href="https://biboamy.github.io/" rel="external nofollow noopener" target="_blank">Yun-Ning Hung</a>, <em>Karn N. Watcharasupat</em>, <a href="https://www.linkedin.com/in/chih-wei-wu-73081689/" rel="external nofollow noopener" target="_blank">Chih-Wei Wu</a>, <a href="https://github.com/ruohoruotsi" rel="external nofollow noopener" target="_blank">Iroro Orife</a>, Kelian Li, Pavan Seshadri, and Junyoung Lee </div> <div class="periodical"> <em>In Extended Abstracts of the Late-Breaking Demo Session of the 22nd International Society for Music Information Retrieval Conference </em> , Nov 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/1510.08484" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="1" data-badge-type="2" data-badge-popover="right" data-altmetric-id="true"></span> <span class="__dimensions_badge_embed__" data-id="true" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=5-5SQ9EAAAAJ&amp;citation_for_view=5-5SQ9EAAAAJ:ufrVoPGSRksC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-2-4285F4?logo=googlescholar&amp;labelColor=beige" alt="2 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p> This report introduces a new corpus of music, speech, and noise. This dataset is suitable for training models for voice activity detection (VAD) and music/speech discrimination. Our corpus is released under a flexible Creative Commons license. The dataset consists of music from several genres, speech from twelve languages, and a wide assortment of technical and non-technical noises. We demonstrate use of this corpus for music/speech discrimination on Broadcast news and VAD for speaker identification. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Hung2021AVASpeechSMADStronglyLabelled</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{
  		AVASpeech-SMAD: A Strongly Labelled Speech and Music Activity Detection
  		Dataset with Label Co-Occurrence
  	}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hung, Yun-Ning and Watcharasupat, Karn N. and Wu, Chih-Wei and Orife, Iroro and Li, Kelian and Seshadri, Pavan and Lee, Junyoung}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{
  		Extended Abstracts of the Late-Breaking Demo Session of the 22nd
  		International Society for Music Information Retrieval Conference
  	}</span><span class="p">,</span>
  <span class="na">google_scholar_id</span> <span class="p">=</span> <span class="s">{ufrVoPGSRksC}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#009f06"> <div>Conference</div> </abbr> </div> <div id="Nguyen2021WhatMakesSound" class="col-sm-8"> <div class="title"> What Makes Sound Event Localization and Detection Difficult? Insights from Error Analysis </div> <div class="author"> <a href="https://sg.linkedin.com/in/nguyen-thi-ngoc-tho-15102012" rel="external nofollow noopener" target="_blank">Thi Ngoc Tho Nguyen</a>, <em>Karn N. Watcharasupat</em>, Zhen Jian Lee , Ngoc Khanh Nguyen , Douglas L Jones, and <a href="https://www3.ntu.edu.sg/home/ewsgan/" rel="external nofollow noopener" target="_blank">Woon Seng Gan</a> </div> <div class="periodical"> <em>In Proceedings of the 6th Workshop on Detection and Classification of Acoustic Scenes and Events </em> , Nov 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="1" data-badge-type="2" data-badge-popover="right" data-altmetric-id="true"></span> <span class="__dimensions_badge_embed__" data-id="true" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=5-5SQ9EAAAAJ&amp;citation_for_view=5-5SQ9EAAAAJ:UeHWp8X0CEIC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-9-4285F4?logo=googlescholar&amp;labelColor=beige" alt="9 Google Scholar citations"> </a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Nguyen2021WhatMakesSound</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{
  		What Makes Sound Event Localization and Detection Difficult? Insights from
  		Error Analysis
  	}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Nguyen, Thi Ngoc Tho and Watcharasupat, Karn N. and Lee, Zhen Jian and Nguyen, Ngoc Khanh and Jones, Douglas L and Gan, Woon Seng}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{
  		Proceedings of the 6th Workshop on Detection and Classification of Acoustic
  		Scenes and Events
  	}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{November}</span><span class="p">,</span>
  <span class="na">google_scholar_id</span> <span class="p">=</span> <span class="s">{UeHWp8X0CEIC}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#009f06"> <div>Conference</div> </abbr> </div> <div id="Karnapi2021DevelopmentFeedbackInterface" class="col-sm-8"> <div class="title">Development of a Feedback Interface for In-Situ Soundscape Evaluation</div> <div class="author"> Furi Andi Karnapi, <a href="https://www.linkedin.com/in/lambhan" rel="external nofollow noopener" target="_blank">Bhan Lam</a>, <a href="https://www.linkedin.com/in/kenneth-ooi-31919b39" rel="external nofollow noopener" target="_blank">Kenneth Ooi</a>, Yun-Ting Lau, <em>Karn Watcharasupat</em>, Trevor Wong, <a href="https://www3.ntu.edu.sg/home/ewsgan/" rel="external nofollow noopener" target="_blank">Woon-Seng Gan</a>, Jooyoung Hong, Samuel Yeong, and Irene Lee </div> <div class="periodical"> <em>In Proceedings of the 50th International Congress and Expo on Noise Control Engineering </em> , Aug 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="1" data-badge-type="2" data-badge-popover="right" data-doi="10.3397/in-2021-2084"></span> <span class="__dimensions_badge_embed__" data-doi="10.3397/in-2021-2084" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=5-5SQ9EAAAAJ&amp;citation_for_view=5-5SQ9EAAAAJ:zYLM7Y9cAGgC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-4-4285F4?logo=googlescholar&amp;labelColor=beige" alt="4 Google Scholar citations"> </a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Karnapi2021DevelopmentFeedbackInterface</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Development of a Feedback Interface for In-Situ Soundscape Evaluation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Karnapi, Furi Andi and Lam, Bhan and Ooi, Kenneth and Lau, Yun-Ting and Watcharasupat, Karn and Wong, Trevor and Gan, Woon-Seng and Hong, Jooyoung and Yeong, Samuel and Lee, Irene}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">aug</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{
  		Proceedings of the 50th International Congress and Expo on Noise Control
  		Engineering
  	}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{I-INCE}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Washington, D.C., USA}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.3397/in-2021-2084}</span><span class="p">,</span>
  <span class="na">google_scholar_id</span> <span class="p">=</span> <span class="s">{zYLM7Y9cAGgC}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#009f06"> <div>Conference</div> </abbr> </div> <div id="Wong2021AssessmentInterICSound" class="col-sm-8"> <div class="title"> Assessment of Inter-IC Sound Microelectromechanical Systems Microphones for Soundscape Reporting </div> <div class="author"> Trevor Wong, <a href="https://www.linkedin.com/in/lambhan" rel="external nofollow noopener" target="_blank">Bhan Lam</a>, <em>Karn Watcharasupat</em>, <a href="https://www.linkedin.com/in/kenneth-ooi-31919b39" rel="external nofollow noopener" target="_blank">Kenneth Ooi</a>, Zhen-Ting Ong, Furi Andi Karnapi, <a href="https://www3.ntu.edu.sg/home/ewsgan/" rel="external nofollow noopener" target="_blank">Woon-Seng Gan</a>, Jooyoung Hong, Samuel Yeong, and Irene Lee </div> <div class="periodical"> <em>In Proceedings of the 50th International Congress and Expo on Noise Control Engineering </em> , Aug 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="1" data-badge-type="2" data-badge-popover="right" data-doi="10.3397/in-2021-2086"></span> <span class="__dimensions_badge_embed__" data-doi="10.3397/in-2021-2086" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=5-5SQ9EAAAAJ&amp;citation_for_view=5-5SQ9EAAAAJ:IjCSPb-OGe4C" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-0-4285F4?logo=googlescholar&amp;labelColor=beige" alt="0 Google Scholar citations"> </a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Wong2021AssessmentInterICSound</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{
  		Assessment of Inter-IC Sound Microelectromechanical Systems Microphones for
  		Soundscape Reporting
  	}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wong, Trevor and Lam, Bhan and Watcharasupat, Karn and Ooi, Kenneth and Ong, Zhen-Ting and Karnapi, Furi Andi and Gan, Woon-Seng and Hong, Jooyoung and Yeong, Samuel and Lee, Irene}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">aug</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{
  		Proceedings of the 50th International Congress and Expo on Noise Control
  		Engineering
  	}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{I-INCE}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Washington, D.C., USA}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.3397/in-2021-2086}</span><span class="p">,</span>
  <span class="na">google_scholar_id</span> <span class="p">=</span> <span class="s">{IjCSPb-OGe4C}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#444444"> <div>Preprint</div> </abbr> </div> <div id="Watcharasupat2021ImprovingPolyphonicSound" class="col-sm-8"> <div class="title"> Improving Polyphonic Sound Event Detection on Multichannel Recordings with the Sørensen-Dice Coefficient Loss and Transfer Learning </div> <div class="author"> <em>Karn N. Watcharasupat</em>, <a href="https://sg.linkedin.com/in/nguyen-thi-ngoc-tho-15102012" rel="external nofollow noopener" target="_blank">Thi Ngoc Tho Nguyen</a> , Ngoc Khanh Nguyen, Zhen Jian Lee , Douglas L Jones, and <a href="https://www3.ntu.edu.sg/home/ewsgan/" rel="external nofollow noopener" target="_blank">Woon Seng Gan</a> </div> <div class="periodical"> Jul 2021 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/2107.10471" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="1" data-badge-type="2" data-badge-popover="right" data-altmetric-id="true"></span> <span class="__dimensions_badge_embed__" data-id="true" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=5-5SQ9EAAAAJ&amp;citation_for_view=5-5SQ9EAAAAJ:qjMakFHDy7sC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-6-4285F4?logo=googlescholar&amp;labelColor=beige" alt="6 Google Scholar citations"> </a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">Watcharasupat2021ImprovingPolyphonicSound</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{
  		Improving {{Polyphonic Sound Event Detection}} on {{Multichannel Recordings}}
  		with the {{S{\o}rensen-Dice Coefficient Loss}} and {{Transfer Learning}}
  	}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Watcharasupat, Karn N. and Nguyen, Thi Ngoc Tho and Nguyen, Ngoc Khanh and Lee, Zhen Jian and Jones, Douglas L and Gan, Woon Seng}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">google_scholar_id</span> <span class="p">=</span> <span class="s">{qjMakFHDy7sC}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Tech Rep.</abbr> </div> <div id="Nguyen2021DCASE2021Task" class="col-sm-8"> <div class="title"> DCASE 2021 Task 3: Spectrotemporally-aligned Features for Polyphonic Sound Event Localization and Detection </div> <div class="author"> <a href="https://sg.linkedin.com/in/nguyen-thi-ngoc-tho-15102012" rel="external nofollow noopener" target="_blank">Thi Ngoc Tho Nguyen</a>, <em>Karn Watcharasupat</em> , Ngoc Khanh Nguyen , Douglas L Jones, and <a href="https://www3.ntu.edu.sg/home/ewsgan/" rel="external nofollow noopener" target="_blank">Woon Seng Gan</a> </div> <div class="periodical"> Jul 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="1" data-badge-type="2" data-badge-popover="right" data-altmetric-id="true"></span> <span class="__dimensions_badge_embed__" data-id="true" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=5-5SQ9EAAAAJ&amp;citation_for_view=5-5SQ9EAAAAJ:2osOgNQ5qMEC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-23-4285F4?logo=googlescholar&amp;labelColor=beige" alt="23 Google Scholar citations"> </a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@techreport</span><span class="p">{</span><span class="nl">Nguyen2021DCASE2021Task</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{
  		DCASE 2021 Task 3: Spectrotemporally-aligned Features for Polyphonic Sound
  		Event Localization and Detection
  	}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Nguyen, Thi Ngoc Tho and Watcharasupat, Karn and Nguyen, Ngoc Khanh and Jones, Douglas L and Gan, Woon Seng}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{
  		IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and
  		Events
  	}</span><span class="p">,</span>
  <span class="na">google_scholar_id</span> <span class="p">=</span> <span class="s">{2osOgNQ5qMEC}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#b509ac"> <div>Patent</div> </abbr> </div> <div id="Nguyen2021MethodAudioProcessingCN" class="col-sm-8"> <div class="title">Blind source separation method without calculating sampling frequency error and audio processing system (不须计算取样频率误差的盲源分离方法以及音频处理系统)</div> <div class="author"> Hai Trieu Anh Nguyen (阮海潮英), Wai Hoong Khong (邝伟雄), Karn Watcharasupat (瓦特察拉苏帕特 甘), and Qing Liu (刘晴) </div> <div class="periodical"> China Patent App. CN202110660272.4A, Filed Jun 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://patents.google.com/patent/CN114999520A" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="1" data-badge-type="2" data-badge-popover="right" data-altmetric-id="true"></span> <span class="__dimensions_badge_embed__" data-id="true" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=5-5SQ9EAAAAJ&amp;citation_for_view=5-5SQ9EAAAAJ:qUcmZB5y_30C" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-0-4285F4?logo=googlescholar&amp;labelColor=beige" alt="0 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>The present disclosure provides a blind source separation method and an audio processing system without calculating a sampling frequency error, wherein the blind source separation method is suitable for an audio processing system, and the audio processing system comprises a plurality of devices, and each device comprises a plurality of microphones. The difference between the signal vector sensed by each device and a row of the mixing matrix is calculated, the difference is used to establish an objective function, and then an optimization algorithm is performed to calculate the mixing matrix. The original signal can be calculated from the mixing matrix and the signal vector without calculating the sampling frequency error between the devices. Thus, there is no need to compensate for the sampling frequency error. 本揭露提出一种不须计算取样频率误差的盲源分离方法以及音频处理系统，盲源分离方法适用于一音频处理系统，此音频处理系统包括多个装置，每一个装置包括多个麦克风。先计算每个装置感测的信号向量与混和矩阵的一行之间的差异，此差异用来建立一目标函数，接着执行一最佳化演算法来计算混和矩阵。根据混和矩阵与信号向量可以计算出原始信号而不用计算装置之间的取样频率误差。如此一来，便不需要补偿取样频率误差。 </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@patent</span><span class="p">{</span><span class="nl">Nguyen2021MethodAudioProcessingCN</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Nguyen (阮海潮英), Hai Trieu Anh and Khong (邝伟雄), Wai Hoong and Watcharasupat (瓦特察拉苏帕特 甘), Karn and Liu (刘晴), Qing}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Blind source separation method without calculating sampling frequency error and audio processing system (不须计算取样频率误差的盲源分离方法以及音频处理系统)}</span><span class="p">,</span>
  <span class="na">nationality</span> <span class="p">=</span> <span class="s">{China}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{CN202110660272.4A}</span><span class="p">,</span>
  <span class="na">dayfiled</span> <span class="p">=</span> <span class="s">{15}</span><span class="p">,</span>
  <span class="na">monthfiled</span> <span class="p">=</span> <span class="nv">jun</span><span class="p">,</span>
  <span class="na">yearfiled</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">day</span> <span class="p">=</span> <span class="s">{15}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jun</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">google_scholar_id</span> <span class="p">=</span> <span class="s">{qUcmZB5y_30C}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#009f06"> <div>Conference</div> </abbr> </div> <div id="Watcharasupat2021DirectionalSparseFiltering" class="col-sm-8"> <div class="title"> Directional Sparse Filtering Using Weighted Lehmer Mean for Blind Separation of Unbalanced Speech Mixtures </div> <div class="author"> <em>Karn Watcharasupat</em> , Anh H. T. Nguyen , Ching-Hui Ooi , and Andy W. H. Khong </div> <div class="periodical"> <em>In Proceedings of the 2021 IEEE International Conference on Acoustics, Speech and Signal Processing </em> , Jun 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2102.00196" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/9414336" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="http://arxiv.org/abs/2102.00196" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="1" data-badge-type="2" data-badge-popover="right" data-doi="10.1109/icassp39728.2021.9414336"></span> <span class="__dimensions_badge_embed__" data-doi="10.1109/icassp39728.2021.9414336" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=5-5SQ9EAAAAJ&amp;citation_for_view=5-5SQ9EAAAAJ:u-x6o8ySG0sC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-2-4285F4?logo=googlescholar&amp;labelColor=beige" alt="2 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p> In blind source separation of speech signals, the inherent imbalance in the source spectrum poses a challenge for methods that rely on single-source dominance for the estimation of the mixing matrix. We propose an algorithm based on the directional sparse filtering (DSF) framework that utilizes the Lehmer mean with learnable weights to adaptively account for source imbalance. Performance evaluation in multiple real acoustic environments show improvements in source separation compared to the baseline methods. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Watcharasupat2021DirectionalSparseFiltering</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{
  		Directional Sparse Filtering Using Weighted Lehmer Mean for Blind Separation
  		of Unbalanced Speech Mixtures
  	}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Watcharasupat, Karn and Nguyen, Anh H. T. and Ooi, Ching-Hui and Khong, Andy W. H.}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jun</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{
  		Proceedings of the 2021 IEEE International Conference on Acoustics, Speech
  		and Signal Processing
  	}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Toronto, Canada}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{4485--4489}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/icassp39728.2021.9414336}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{978-1-72817-605-5}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{23318422}</span><span class="p">,</span>
  <span class="na">google_scholar_id</span> <span class="p">=</span> <span class="s">{u-x6o8ySG0sC}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#444444"> <div>Preprint</div> </abbr> </div> <div id="Watcharasupat2020VisualAttentionMusical" class="col-sm-8"> <div class="title">Visual Attention for Musical Instrument Recognition</div> <div class="author"> <em>Karn Watcharasupat</em>, Siddharth Gururani, and <a href="https://www.alexanderlerch.com/" rel="external nofollow noopener" target="_blank">Alexander Lerch</a> </div> <div class="periodical"> May 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2006.09640" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="1" data-badge-type="2" data-badge-popover="right" data-altmetric-id="true"></span> <span class="__dimensions_badge_embed__" data-id="true" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=5-5SQ9EAAAAJ&amp;citation_for_view=5-5SQ9EAAAAJ:u5HHmVD_uO8C" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-7-4285F4?logo=googlescholar&amp;labelColor=beige" alt="7 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p> In the field of music information retrieval, the task of simultaneously identifying the presence or absence of multiple musical instruments in a polyphonic recording remains a hard problem. Previous works have seen some success in improving instrument classification by applying temporal attention in a multi-instance multi-label setting, while another series of work has also suggested the role of pitch and timbre in improving instrument recognition performance. In this project, we further explore the use of attention mechanism in a timbral-temporal sense, la visual attention, to improve the performance of musical instrument recognition using weakly-labeled data. Two approaches to this task have been explored. The first approach applies attention mechanism to the sliding-window paradigm, where a prediction based on each timbral-temporal ‘instance’ is given an attention weight, before aggregation to produce the final prediction. The second approach is based on a recurrent model of visual attention where the network only attends to parts of the spectrogram and decide where to attend to next, given a limited number of ‘glimpses’. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">Watcharasupat2020VisualAttentionMusical</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Visual Attention for Musical Instrument Recognition}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Watcharasupat, Karn and Gururani, Siddharth and Lerch, Alexander}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">google_scholar_id</span> <span class="p">=</span> <span class="s">{u5HHmVD_uO8C}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 karn watcharasupat. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-0CJ2WMKNQY"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-0CJ2WMKNQY");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>